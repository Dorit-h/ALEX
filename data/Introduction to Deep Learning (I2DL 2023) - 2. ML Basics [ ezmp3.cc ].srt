1
00:00:00,960 --> 00:00:06,880
Welcome everybody to Introduction to Deep Learning. This is the second lecture and in this

2
00:00:06,880 --> 00:00:13,520
lecture we're going to start with the machine learning basics. If we are remembering very

3
00:00:13,520 --> 00:00:19,200
briefly from the previous lecture we gave a very high level overview of what this lecture content

4
00:00:19,200 --> 00:00:28,720
is going to be and to be very precise I wanted to give a differentiation between AI, ML and DL,

5
00:00:28,720 --> 00:00:32,080
artificial intelligence, machine learning and deep learning and what the differences are.

6
00:00:32,880 --> 00:00:39,520
And this is exactly where we would like to continue today and specifically what we have

7
00:00:39,520 --> 00:00:45,200
discussed was that artificial intelligence was pretty much anything we can write up in a program

8
00:00:45,200 --> 00:00:50,800
even if statement you could in some way consider artificial intelligence and machine learning is

9
00:00:50,800 --> 00:00:57,680
then the next well more sophisticated step where we have kind of a model that is being optimized

10
00:00:57,680 --> 00:00:58,640
and is learning based on AI and DL. So let's get started.

11
00:00:58,720 --> 00:01:02,880
So we have a model that is based on some training data in order to perform a certain task and then

12
00:01:02,880 --> 00:01:07,680
finally we're going to talk about deep learning where we're going to have a neural network as

13
00:01:07,680 --> 00:01:12,640
our model that is then well supposedly more powerful than alternative machine learning

14
00:01:12,640 --> 00:01:20,320
methods. Okay and this is exactly where we would like to continue today and before we step into

15
00:01:20,320 --> 00:01:25,040
these three categories again I would now have the orthogonal thing when we're talking about tasks.

16
00:01:25,040 --> 00:01:28,640
So what I've just talked about was methods right what are learning methods, what are

17
00:01:28,720 --> 00:01:32,960
artificial intelligence methods, algorithms and now we're going to talk about some tasks.

18
00:01:32,960 --> 00:01:37,280
So mainly we want to apply these kind of methods that we have in these categories here.

19
00:01:37,280 --> 00:01:42,160
We want to apply them for instance to computer vision and we would like to start with a very very

20
00:01:42,160 --> 00:01:48,080
simple task here and a simple task could be image classification right. This is the task we're going

21
00:01:48,080 --> 00:01:55,680
to deal with for quite a bit because it's very canonical and it's very practical for

22
00:01:55,680 --> 00:01:58,480
computer vision applications. So what is it about?

23
00:01:58,720 --> 00:02:03,680
It's relatively straightforward. So the task here is essentially give them a set of input images.

24
00:02:04,800 --> 00:02:10,240
What we would like to do is we would like to assign a class label for each of these images

25
00:02:10,240 --> 00:02:16,160
right. So each of these cats here we would like to assign an orange box and each of the dogs here

26
00:02:16,160 --> 00:02:20,880
we would like to assign a blue box. And ideally the way we would like to do this is we have been

27
00:02:20,880 --> 00:02:26,240
feeding this respective image that we want to look at and that we don't know the label yet.

28
00:02:26,240 --> 00:02:28,400
We would like to make a prediction based

29
00:02:28,720 --> 00:02:33,840
on a certain model that we're having for instance right. And this is the whole point of image

30
00:02:33,840 --> 00:02:41,920
classification right. And the idea behind that is basically that we now in a sense having a mapping

31
00:02:41,920 --> 00:02:49,520
from the input which is for instance a set of pixels to some form of output label right. So we

32
00:02:49,520 --> 00:02:58,480
have to find a mapping from a bunch of pixels of this image to which output label is going to be.

33
00:02:59,440 --> 00:03:06,160
And well you could you could imagine why this is becoming relatively complicated because the

34
00:03:06,160 --> 00:03:11,520
diversity in natural images is actually quite complicated. To be a little bit more precise here

35
00:03:12,640 --> 00:03:17,840
we see already that we have quite a high diversity here. But even these images that you see here just

36
00:03:17,840 --> 00:03:23,600
for two simple classes for dogs and cats here you see that they're also relatively straightforward.

37
00:03:23,600 --> 00:03:28,720
Meaning that in each of these images there's a unique label that can be associated to them.

38
00:03:28,720 --> 00:03:33,360
Meaning that there's no mixture of like dogs and cats in the same image. So this is already

39
00:03:33,360 --> 00:03:38,560
a much more simplified task than if you're considering like real world applications.

40
00:03:39,360 --> 00:03:44,480
But even that simple task of image classification where it's a little bit easier. Even that one is

41
00:03:44,480 --> 00:03:50,240
already pretty complicated and worth a look that we will devote quite a bit of time during this

42
00:03:50,240 --> 00:03:56,640
lecture. But why is that even so complicated? And it's actually very straightforward if you're

43
00:03:56,640 --> 00:03:58,560
looking at these images. So for instance if you take a very simple single project image here for instance, so this is one

44
00:03:58,560 --> 00:04:03,680
instance if you're taking these kind of images here you see very briefly that we have a lot of

45
00:04:03,680 --> 00:04:09,440
occlusions meaning that the problem that we're dealing with practically speaking here is that

46
00:04:09,440 --> 00:04:14,180
the real world around us is actually in three dimensions but what the image is actually

47
00:04:14,180 --> 00:04:18,480
portraying and visualizing is just based on two dimensions meaning we have kind of like

48
00:04:18,480 --> 00:04:25,240
like the the camera that took this image took the 3d world and made it 2d and in this projection

49
00:04:25,240 --> 00:04:30,700
the this is not a bijective mapping anymore right so we're losing some information from the 3d world

50
00:04:30,700 --> 00:04:37,460
and we're going to 2d and because of that we can have many many 2d images for the same 3d objects

51
00:04:37,460 --> 00:04:43,880
underneath right and this is a big problem meaning that the resulting well output here has things

52
00:04:43,880 --> 00:04:48,160
like occlusions right so not all the dog here is always visible depending on from which camera

53
00:04:48,160 --> 00:04:53,840
angle the image was taken and each of these images you can also see they have some stuff in it so

54
00:04:53,840 --> 00:04:55,220
this image has a little box in it for example and this is a little box in it for example

55
00:04:55,220 --> 00:04:55,240
so this image has a little box in it for example and this image has a little box in it for example

56
00:04:55,240 --> 00:05:01,160
instance um and and so on so right so these occlusions make it very difficult adding more

57
00:05:01,160 --> 00:05:07,640
complexity to the respective um yeah diversity of images we can also see that the backgrounds

58
00:05:07,640 --> 00:05:11,800
can be vastly different right and this makes a big difference we can have very extreme cases

59
00:05:11,800 --> 00:05:15,720
like these two here right so we can see here there's a white background with the white deck and

60
00:05:15,720 --> 00:05:20,520
uh white dog in the foreground um and they have here a black cat during the night with

61
00:05:20,520 --> 00:05:25,220
a black background right so this is kind of an extreme example why image classification

62
00:05:25,220 --> 00:05:31,780
could become very tricky and why especially for computers it's very difficult to make distinctions

63
00:05:31,780 --> 00:05:36,940
or to assign correct class labels in these specific cases here um and if you're just

64
00:05:36,940 --> 00:05:42,100
continuing this what i've just talked about for instance you're going to you know just go in

65
00:05:42,100 --> 00:05:47,800
google image search and you're trying to find for look for a bunch of cat images you see kind of a

66
00:05:47,800 --> 00:05:55,200
large diversity um in terms of images um so you see very different poses how is the cat

67
00:05:55,200 --> 00:06:00,660
looking what is the current pose like this is one this this kitten is kind of like in this up pose

68
00:06:00,660 --> 00:06:07,020
here right um this cat here sitting down um this cat is like to the side so the cat itself can have

69
00:06:07,020 --> 00:06:12,260
a different pose but also the camera what i just mentioned is because again we go from this 3d world

70
00:06:12,260 --> 00:06:17,980
to the 2d image can also have a different pose right the camera pose here our camera parameters

71
00:06:17,980 --> 00:06:23,580
can vastly change the camera in itself can also change by the way right so we can go ahead um and

72
00:06:23,580 --> 00:06:25,200
have and connect the camera to the camera itself so we can go ahead and have and connect the camera

73
00:06:25,200 --> 00:06:30,000
we can have different lenses we can have different focal lengths like simple autofocus or so might

74
00:06:30,000 --> 00:06:35,680
actually change quite a bit how the image is going to look like um illumination can change um that's

75
00:06:35,680 --> 00:06:41,460
very obvious right and illumination can be quite complicated right day night we have just seen

76
00:06:41,460 --> 00:06:45,960
um but we can also have like vastly different environments basically where we see

77
00:06:45,960 --> 00:06:51,080
um respectively different outputs and then of course we have different appearances with very

78
00:06:51,080 --> 00:06:55,120
very extreme cases where like have a cat is kind of wrapped um like a

79
00:06:55,200 --> 00:07:01,200
burrito and in that case of course it's very difficult to do a proper representation so if

80
00:07:01,200 --> 00:07:07,200
you're summarizing most of these things um and a lot of people who have taken for instance computer

81
00:07:07,200 --> 00:07:11,340
graphics courses they know basically the problem that we're dealing with here right so we're

82
00:07:11,340 --> 00:07:16,740
basically dealing with the 3d world um we have a bunch of in graphics you would say scene parameters

83
00:07:16,740 --> 00:07:21,300
right you have poses where the camera is located you're going to have light sources being

84
00:07:21,300 --> 00:07:24,900
distributed in a scene you're going to apply some shading like

85
00:07:25,200 --> 00:07:30,380
how do the respective materials reflect and so on so all the things that you feed in the graphics

86
00:07:30,380 --> 00:07:34,440
pipeline they're going to tell you at the end of the day how an image is going to look like and as

87
00:07:34,440 --> 00:07:38,400
i said this is not a bi-checked mapping meaning that there's many many images that actually

88
00:07:38,400 --> 00:07:45,000
explain the same 3d underlying project uh object and even what makes matters worse even the 3d

89
00:07:45,000 --> 00:07:51,660
shapes within one class are even diverse too so we have different 3d shapes and each 3d shape

90
00:07:51,660 --> 00:07:55,000
can have different images associated to it and that problem

91
00:07:55,200 --> 00:08:00,900
basically means that well this becomes a pretty difficult problem because for a bunch of like cats

92
00:08:00,900 --> 00:08:07,380
we actually can have quite a large diversity of images and this makes image classification such a

93
00:08:07,380 --> 00:08:13,380
challenging but also interesting problem that we would like to look at one of the key things that

94
00:08:13,380 --> 00:08:17,820
we would also like to talk about in this lecture is what is the actual representation and this is

95
00:08:17,820 --> 00:08:23,820
a really really big question um when i'm talking about classification um the representation for the

96
00:08:23,820 --> 00:08:24,840
most part we're going to use

97
00:08:25,200 --> 00:08:34,500
um as input is going to be a set of pixels it's a 2d array right we have rgb d values maybe probably

98
00:08:34,500 --> 00:08:40,560
not d but most of the time we have rgb depth is not always given um sometimes it's just grayscale

99
00:08:40,560 --> 00:08:46,320
sometimes rgb so these are typically our input representations that we're giving for instance

100
00:08:47,520 --> 00:08:52,620
um when we're looking at a jpeg or a png file but then what we would like to do is we would like to

101
00:08:52,620 --> 00:08:55,140
create higher level representations

102
00:08:55,200 --> 00:08:58,740
and this is the thing we're going to talk a little about and we're going to talk about feature

103
00:08:58,740 --> 00:09:03,720
extraction we would like to talk about representations that basically abstract all

104
00:09:03,720 --> 00:09:08,400
these things away what i've just talked about right a good representation would be that all

105
00:09:08,400 --> 00:09:13,500
of these different things here shouldn't matter for a good representation so this raw representation

106
00:09:13,500 --> 00:09:19,560
of input pixels we would ideally like to map to some high level representation where it's much

107
00:09:19,560 --> 00:09:24,900
easier to do classification um and this is something that we're going to use little networks

108
00:09:25,200 --> 00:09:29,940
um but unfortunately there's still a few things we have to learn first we have to look at a few

109
00:09:29,940 --> 00:09:35,940
things before we actually start with complicated feature learning um instead what we would like to

110
00:09:35,940 --> 00:09:41,280
do is we would like to start a little bit simpler um in fact we would like to start very very soon

111
00:09:41,280 --> 00:09:48,060
so the simplest possible way or the simplest let's say algorithm what we could deal with

112
00:09:48,840 --> 00:09:55,080
um is a very simple classifier and the way what we could do here is

113
00:09:55,200 --> 00:10:01,620
we could do simply nearest neighbor lookups so what does that mean well let's assume we're

114
00:10:01,620 --> 00:10:08,040
having a bunch of images let's also assume um for these images we're going to have labels so

115
00:10:08,040 --> 00:10:12,540
we know their respective classes right so for instance we have these images here uh for these

116
00:10:12,540 --> 00:10:17,400
images we're having one two three four five six cat images we're going to label them with an

117
00:10:17,400 --> 00:10:22,920
orange box this makes it clear that this is an orange box um that refers to cat uh we're going

118
00:10:22,920 --> 00:10:24,840
to have two dogs here they have blue boxes

119
00:10:25,200 --> 00:10:31,920
which is referring to a dog and now what we would like to do is we would consider these ones as our

120
00:10:31,920 --> 00:10:39,060
reference images um reference images is is maybe not the right term the reason why I'm using it is

121
00:10:39,060 --> 00:10:43,080
later we're going to call them training images but at the moment there's no training at the moment

122
00:10:43,080 --> 00:10:47,820
all we would like to do is we would like to take these reference images here um and what we would

123
00:10:47,820 --> 00:10:53,280
like to do is we would like to take a new image here and for this new image we would like to

124
00:10:53,280 --> 00:10:53,700
understand

125
00:10:54,540 --> 00:10:55,140
to

126
00:10:55,200 --> 00:11:06,540
the closest hence the nearest neighbor so which of these other images is our nearest neighbor well

127
00:11:06,540 --> 00:11:11,460
so how do we do that well it comes a little bit back to the question what we've addressed before

128
00:11:11,460 --> 00:11:17,340
what is the right representation so in any kind of representation we would love to compute a distance

129
00:11:17,340 --> 00:11:23,100
in other words what we would love to do is we would take this image and compare it against every

130
00:11:23,100 --> 00:11:24,540
other image here right

131
00:11:24,540 --> 00:11:28,440
right so let's just say we can do that let's just say we have this magic function right now

132
00:11:28,440 --> 00:11:36,300
in how to make that comparison so we take this image compare it computer value this image that

133
00:11:36,300 --> 00:11:43,020
image compare the value and so on and then what we're going to do is we're just going to go ahead

134
00:11:43,020 --> 00:11:49,920
and sort all these images in our reference data set by these distances we just computed

135
00:11:50,700 --> 00:11:53,880
um and we might end up getting something like this right so here we have our reference image

136
00:11:53,880 --> 00:12:00,120
sorry here we have our query image and here we have all these reference images and now we're going

137
00:12:00,120 --> 00:12:05,400
to have um a distance function to each of these ones we're going to get a distance value and we

138
00:12:05,400 --> 00:12:10,740
sort this by distance so these things here this dog image is supposedly to be the closest and

139
00:12:10,740 --> 00:12:16,380
these cat image here they're supposed to be the furthest distant away from this query image that

140
00:12:16,380 --> 00:12:22,800
we have here right and the idea behind this nearest neighbor classifier is after we have computed the

141
00:12:22,800 --> 00:12:23,220
distances

142
00:12:23,220 --> 00:12:29,940
and after we have sorted them by the distance we simply assign the class level to the closest one

143
00:12:29,940 --> 00:12:35,460
so in other words well okay this one is presumably the closest one uh and in this case our nearest

144
00:12:35,460 --> 00:12:42,180
neighbor classifier would simply say well this is a dog right um and that's it now

145
00:12:43,140 --> 00:12:47,880
um you might imagine of course there's a couple of drawbacks you might imagine it could be a large

146
00:12:47,880 --> 00:12:53,160
data set there's a lot of compute involved but it could also be pretty noisy it might be accidentally

147
00:12:53,220 --> 00:13:00,060
that there's like one image of a cat that looks surprisingly similar um and we would like to be

148
00:13:00,060 --> 00:13:05,460
a little bit more robust so instead of saying oh I'm going to try to be the closest to this one dog

149
00:13:05,460 --> 00:13:10,860
here or to this closest image I'm going to say I'm going to just take some sort of a median in

150
00:13:10,860 --> 00:13:15,300
a sense right I'm going to go ahead and say I'm going to take the multiple nearest neighbors and

151
00:13:15,300 --> 00:13:21,660
then simply do a majority vote which once is the majority in the k nearest neighbors so for instance

152
00:13:21,660 --> 00:13:23,100
if we're saying we're setting this

153
00:13:23,220 --> 00:13:29,340
k like this nearest neighbor classifier to a three-way nearest neighbor classifier so k nearest

154
00:13:29,340 --> 00:13:37,560
neighbors where we said k to three um and we take the three closest images here one two three uh and

155
00:13:37,560 --> 00:13:42,060
in this case we would say it's a cat actually right because now we have two images here that

156
00:13:42,060 --> 00:13:49,140
are referring to a cat and um one image that refers to dog so our majority vote here told us

157
00:13:49,140 --> 00:13:52,560
this is a cat of course this is wrong right in this case our

158
00:13:53,220 --> 00:13:58,320
main nearest neighbor classifier was not that great uh but could you can imagine how this

159
00:13:58,320 --> 00:14:04,860
basically changes right so you could imagine if we had a larger set of case we would get a little

160
00:14:04,860 --> 00:14:11,760
bit more robust to the respective data right okay um what's kind of interesting is you can actually

161
00:14:11,760 --> 00:14:17,880
go ahead and take this classifier um and plot it in 2D so in other words what we would love to do

162
00:14:18,780 --> 00:14:23,040
uh we would simply take our data these are our reference images presumably

163
00:14:23,220 --> 00:14:28,020
somehow um and now what we want to do is we want to assign like you want to kind of extrapolate we

164
00:14:28,020 --> 00:14:31,920
want to make sure that the entirety of this 2D area here is going to be filled with associated

165
00:14:31,920 --> 00:14:38,040
labels so no matter where you are in this 2D space you want to figure out what the respective nearest

166
00:14:38,040 --> 00:14:42,060
neighbors are so if I'm here I'm taking the nearest point for instance this one then I

167
00:14:42,060 --> 00:14:47,040
will assign a red here so this this point everything here is basically closest to this

168
00:14:47,040 --> 00:14:52,620
one right everything here in this area is close to this red point and so on so this is just literally

169
00:14:53,220 --> 00:15:00,240
the areas based on which reference image was the closest um and this is pretty straightforward all

170
00:15:00,240 --> 00:15:06,180
you're doing is you're trying to find um the nearest neighbor point from where you are and

171
00:15:06,180 --> 00:15:10,320
then you assign the respective class to where you are at the moment right and this of course

172
00:15:10,320 --> 00:15:15,480
this changes now if you're going from a one nearest neighbor classifier to only taking the

173
00:15:15,480 --> 00:15:21,360
nearest one to a five-way nearest neighbor classifier so from a five-way nearest neighbor

174
00:15:23,220 --> 00:15:33,360
we are trying to figure out um which one like whether it's a little bit more robust so in this

175
00:15:33,360 --> 00:15:37,860
case for instance there's a green point here right you see this so don't confuse my laser pointer

176
00:15:37,860 --> 00:15:42,840
with point um because I have a red laser pointer but there's a green point here uh and here we

177
00:15:42,840 --> 00:15:47,880
assign this one this area here was the closest to this green point and here since we're saying

178
00:15:47,880 --> 00:15:53,160
we have five and nearest neighbor classifier um that means this this green point you get

179
00:15:53,220 --> 00:16:00,360
ignored right so this outlier gets um gets ignored for the time being um and if you have

180
00:16:00,360 --> 00:16:04,740
one outlier in our metric here that means our nearest neighbor classified gets slightly worse

181
00:16:05,400 --> 00:16:10,020
not worse but it's um like here we're getting slightly better because we have five and be

182
00:16:10,020 --> 00:16:16,020
ignoring it and supposed to be an outlet okay so let's have a bit of a discussion what the

183
00:16:16,020 --> 00:16:22,440
implications are between one nearest neighbor five nearest neighbor and so on so let's have a couple

184
00:16:22,440 --> 00:16:23,160
of um

185
00:16:23,220 --> 00:16:27,420
um interesting questions so the first interesting question is well I called this previously the

186
00:16:27,420 --> 00:16:32,280
reference data set in practice that's going to be our training data right uh in the end of the day

187
00:16:32,280 --> 00:16:36,960
we want to train something on it um so if you're performing the nearest neighbor classifier on the

188
00:16:36,960 --> 00:16:41,580
training data how would this perform on the training data right so this is kind of a trick

189
00:16:41,580 --> 00:16:48,480
question because if we're trying to find the closest image and the current image that we're

190
00:16:48,480 --> 00:16:53,160
querying is actually in the training data we would assume that the

191
00:16:53,220 --> 00:17:00,960
query image finds the perfect reference image which finds itself right in the training in this

192
00:17:00,960 --> 00:17:07,920
case you will assume that a nearest neighbor classifier would perform perfectly right it

193
00:17:07,920 --> 00:17:12,540
would always find the right image and it would because it finds the right image it would also

194
00:17:12,540 --> 00:17:17,040
assign the right class assuming of course our label is correct here we would make the correct

195
00:17:17,040 --> 00:17:23,100
classification if you're taking a five and a nearest neighbor classifier that's always the case

196
00:17:23,220 --> 00:17:28,260
right so these green points here these green points would not perform perfectly because they

197
00:17:28,260 --> 00:17:32,880
would still be assigned to a red color meaning that they would get the wrong label and even

198
00:17:32,880 --> 00:17:37,380
though they are performed on the training set they would not be perfect anymore so you can already see

199
00:17:37,380 --> 00:17:43,020
that the nearest neighbor classifier and the 5nn classifier they they have slightly different

200
00:17:43,020 --> 00:17:46,740
behavior and this is something we're going to later on pick up a little bit we'll be talking

201
00:17:46,740 --> 00:17:51,960
about robustness in the context of neural networks like for instance how much do we want to regularize

202
00:17:51,960 --> 00:17:53,160
how much we want to smooth the

203
00:17:53,220 --> 00:17:58,680
labels and stuff like that this will become very relevant um the second question you want to ask

204
00:17:58,680 --> 00:18:06,780
here is which classifier is likely to perform best on the test data well it depends right so

205
00:18:06,780 --> 00:18:12,780
the one argument for having a larger K here is we are a little bit better with respect to outliers

206
00:18:12,780 --> 00:18:17,760
and with respect to outliers means we actually have a better coverage of the training samples

207
00:18:17,760 --> 00:18:23,160
and assuming our distribution is matching better between the training these reference

208
00:18:23,160 --> 00:18:27,840
images that we had and the respective query image so in this case we would probably assume

209
00:18:27,840 --> 00:18:33,960
that the the 5nn classifier would perform slightly better but there's a caveat I'm

210
00:18:33,960 --> 00:18:40,620
going to give you an extreme example so if we are for instance setting our K equal to

211
00:18:40,620 --> 00:18:48,360
the total number of images in the training set right um what that would mean is we always going

212
00:18:48,360 --> 00:18:53,100
to take every image into account and just do a majority vote of all the images in our training

213
00:18:53,160 --> 00:18:59,460
set and if that's the case we're just going to always predict the same label right so that's not

214
00:18:59,460 --> 00:19:05,520
going to be very accurate in fact well it's going to just predict the majority class um and it's not

215
00:19:05,520 --> 00:19:09,300
a great example so in this case like if you're choosing for instance this K too large that might

216
00:19:09,300 --> 00:19:14,940
also be as loud downside so this is something also we have to consider and later on when we're doing

217
00:19:14,940 --> 00:19:22,860
neural network optimization uh we can also follow the similar pitfalls here where we would accidentally

218
00:19:23,160 --> 00:19:27,240
predict the majority class so this is always a good a good sanity check whenever we're doing

219
00:19:27,240 --> 00:19:32,520
classifier training we want to do better than the majority class it's not just the average right if

220
00:19:32,520 --> 00:19:38,220
you have more samples from one class we would have to figure out to be better than this one class please

221
00:19:38,220 --> 00:19:45,600
to do anything um the next interesting question like what are we actually learning here right

222
00:19:45,600 --> 00:19:51,120
remember we've talked about AI versus machine learning versus deep learning so if you want

223
00:19:51,120 --> 00:19:53,100
to categorize our nearest neighbor classifier

224
00:19:53,160 --> 00:20:00,780
um well so far if you're thinking carefully we haven't really done a lot of learning here right

225
00:20:00,780 --> 00:20:07,320
there's no like parameters that are trained based on some training data and in fact we

226
00:20:07,920 --> 00:20:12,900
we more or less hard-coded our training sets okay we do have a training set right

227
00:20:13,800 --> 00:20:18,600
um but we didn't train any network here right um or we didn't train any model in this case

228
00:20:18,600 --> 00:20:22,740
and in practice we just hard-coded our samples and we tried to assign it

229
00:20:23,340 --> 00:20:29,460
um and hard-coded in our decision criteria right so which one is closer and based on that we made

230
00:20:29,460 --> 00:20:35,640
a decision so there's no actual learning in a sense going on right um however there's a few

231
00:20:35,640 --> 00:20:40,860
things we can actually play around with um and these things are some hyper parameters

232
00:20:42,300 --> 00:20:46,440
um and higher parameters is a term if you haven't heard it um that's a term that we're going to

233
00:20:46,440 --> 00:20:51,660
mention a lot these are basically parameters that are not part of our model in this case we don't

234
00:20:51,660 --> 00:20:53,040
have a model so there's no other

235
00:20:53,660 --> 00:21:01,020
parameters and these are parameters that are choices for the algorithm itself okay so for

236
00:21:01,020 --> 00:21:05,400
instance in this case hyper parameters is something like which distance function we're going to choose

237
00:21:05,400 --> 00:21:10,260
so the simplest distance function we could choose for our nearest neighbor classifier could be an

238
00:21:10,260 --> 00:21:15,120
L1 distance right so we're just going to take um the pixel values we're going to compare them

239
00:21:15,120 --> 00:21:19,260
one by one and we're going to just compute an L1 distance between the r2p values and that's

240
00:21:19,260 --> 00:21:20,100
going to give us the distance

241
00:21:20,100 --> 00:21:21,380
uh we can also do the same thing with an L2P

242
00:21:21,380 --> 00:21:22,980
we can also do the same thing with an L2P

243
00:21:23,160 --> 00:21:31,000
distance we can choose and decide depending on how we feel like and what we feel works better

244
00:21:32,200 --> 00:21:36,280
and to get different distance function and based on the distance function you

245
00:21:36,280 --> 00:21:42,360
hopefully can also change the outcome and the behavior of our respective classifier

246
00:21:43,880 --> 00:21:48,200
we can also change the number of neighbors right so this k as we have noticed is quite

247
00:21:48,200 --> 00:21:55,800
um quite important um and and this is kind of the the typically higher parameters we would get

248
00:21:55,800 --> 00:22:02,680
in a nearest neighbor classifier now these parameters they are very problem dependent right

249
00:22:03,240 --> 00:22:08,680
so we don't know right depends on what kind of classifier we have what data set we have

250
00:22:09,400 --> 00:22:13,720
we would like to change these ones and now you can already see well now there's

251
00:22:13,720 --> 00:22:16,840
a bit of a concept of learning coming into play right

252
00:22:17,400 --> 00:22:18,120
and by learning

253
00:22:18,200 --> 00:22:22,680
coming into play means well now we would like to look at our data set in this case our training

254
00:22:22,680 --> 00:22:29,880
data set or our reference data set what i called it before um and would like to learn or adjust

255
00:22:29,880 --> 00:22:35,880
these parameters to a certain level um so the big question is how do we actually choose these

256
00:22:35,880 --> 00:22:41,480
hyper parameters right um and now machine learning comes into play right now we want to move from

257
00:22:41,480 --> 00:22:48,120
a nearest neighbor classifier to using machine learning methods in order to learn these classes

258
00:22:48,200 --> 00:22:54,680
okay now what we do here is we would like to perform image classification right so in this

259
00:22:54,680 --> 00:22:59,800
case we have our task we've just defined that one and when we're doing machine learning here we

260
00:22:59,800 --> 00:23:06,760
would like to take a bunch of data experiences we would like to take these ones and we would

261
00:23:06,760 --> 00:23:12,600
like to learn from these ones to learn a model and i'm trying to formalize this a little bit so don't

262
00:23:12,600 --> 00:23:17,560
be too nitpicky here in the formulation um this formulation will not be consistent with the rest of

263
00:23:18,200 --> 00:23:22,040
the model but the high level concept is always going to be the same for every machine learning

264
00:23:23,400 --> 00:23:27,640
for every machine learning problem and again this is a classifier right now but the high level

265
00:23:27,640 --> 00:23:34,040
concept of you having a a certain model you want to find parameters for that model and for the

266
00:23:34,040 --> 00:23:39,320
input you want to make a certain prediction so i want to make clear what i mean by this okay so

267
00:23:39,320 --> 00:23:45,480
here's our model so this is our model m here this model m here has a bunch of parameters

268
00:23:46,840 --> 00:23:48,120
it takes an images input

269
00:23:48,200 --> 00:23:52,680
and it makes a class label prediction right like i'm not going to go into detail like

270
00:23:52,680 --> 00:23:56,520
how the class labels are being encoded right now or how the images are encoded

271
00:23:56,520 --> 00:24:01,880
i wanted to go a step back right make this very high level just get the high level concept of

272
00:24:01,880 --> 00:24:06,760
machine learning and if you know machine learning already i'm sure this is really a good repetition

273
00:24:06,760 --> 00:24:12,040
because this is really a concept that is always going to be shared across many families of

274
00:24:12,040 --> 00:24:17,720
machine learning methods right okay um so yeah so what we have is we have the model m right we have

275
00:24:18,200 --> 00:24:24,440
parameters theta we have an input image i and we have two output labels class labels dog or cat

276
00:24:25,480 --> 00:24:31,640
note i'm not having any hybrid parameters here this theta these are the model parameters this

277
00:24:31,640 --> 00:24:35,240
is very different from the hybrid parameters and we'll later see exactly what that means

278
00:24:35,240 --> 00:24:39,400
but practically as i said the model parameters they're being optimized on the training set

279
00:24:39,400 --> 00:24:43,000
and the hybrid parameters this is a sign choice on the algorithm itself

280
00:24:43,000 --> 00:24:47,800
how we do for instance optimization of these parameters or how big our model is going to be

281
00:24:48,200 --> 00:24:54,120
so we have a lot of hybrid parameters but now what we would love to do is we would like to figure out

282
00:24:54,120 --> 00:25:00,840
if you're taking a bunch of images um we would like to make sure that our model in some sense

283
00:25:02,280 --> 00:25:09,640
follows these predictions what these images have been annotated so in other words if i have a lot

284
00:25:09,640 --> 00:25:17,320
of images um and these images here for this image is the dog if i'm feeding this image i here um this

285
00:25:17,320 --> 00:25:18,040
dog image

286
00:25:18,200 --> 00:25:23,080
here when i feed this image in i would like to make it a dog prediction on the other hand if

287
00:25:23,080 --> 00:25:28,040
i feed this cat image in i would like to make it a cat prediction and now the whole point of machine

288
00:25:28,040 --> 00:25:32,520
learning is that we're starting with a data set like this one and now what we're doing is we're

289
00:25:32,520 --> 00:25:38,040
taking a part of these images um and i've already mentioned it a couple of times because i'm so

290
00:25:38,040 --> 00:25:43,960
used to it um so these images this part of these images we're going to call the train set so we're

291
00:25:43,960 --> 00:25:48,120
going to take given these eye images with training labels so these are the images we're going to start

292
00:25:48,200 --> 00:25:56,280
some training and we would like to do is we would like to make sure that our model makes these

293
00:25:56,280 --> 00:26:03,880
predictions that is in the trendset and then what we're hoping is assuming we have we have figured

294
00:26:03,880 --> 00:26:09,720
out our model based on these images that our model also makes correct predictions on previously

295
00:26:09,720 --> 00:26:13,880
unseen images so if i'm training on these ones i hope that the model makes also good

296
00:26:13,880 --> 00:26:18,040
predictions on these ones ideally perfect predictions right so

297
00:26:18,200 --> 00:26:22,680
this is our training uh train set here and this is something we're going to call the validation set

298
00:26:24,360 --> 00:26:27,960
and the way this is going to go is when i say training

299
00:26:28,680 --> 00:26:33,960
for the most part and again don't go too much into my notation here i just wanted to make this

300
00:26:33,960 --> 00:26:39,640
very very dead simple um what we're going to do is we're going to find these parameters data

301
00:26:40,280 --> 00:26:47,640
and in this case what we do here is we optimizing for a theta such that um

302
00:26:48,200 --> 00:26:52,920
we're going to have a data set here that's going to make the prediction of the model

303
00:26:52,920 --> 00:26:57,800
and what this prediction looks like so if we sum over our training images

304
00:26:58,360 --> 00:27:01,560
our model here if we're summing over all of our training images

305
00:27:02,440 --> 00:27:09,400
minimize the distance between the labels and what the model predicts so model predictions

306
00:27:09,400 --> 00:27:12,680
and ground truth labels these are the ground truth labels cat dog and dog

307
00:27:13,400 --> 00:27:18,120
these ones should match right so if i feel an image i i want to make the label prediction of this image i

308
00:27:18,200 --> 00:27:22,440
function. There's a lot of different ways how to define these distance functions. We're going to

309
00:27:22,440 --> 00:27:28,040
talk about a couple of these ones today and presumably going to talk about a couple of more

310
00:27:28,040 --> 00:27:34,120
in the next lectures as well. Another question is how are these labels even defined? Are these

311
00:27:34,120 --> 00:27:38,360
literally numbers like 0 and 1? There could be also a couple of different ways how to define

312
00:27:38,360 --> 00:27:43,320
those numbers. And then how is the image I define? I mean pixels is the obvious one but could be also

313
00:27:43,320 --> 00:27:49,720
all the things. But the high level concept between machine learning is always going to be the same.

314
00:27:49,720 --> 00:27:56,600
It's always very similar. So what you're doing is you're having some form of a training set.

315
00:27:56,600 --> 00:28:03,720
You're taking these labels here in the ground truth labels and you're trying to make sure the

316
00:28:03,720 --> 00:28:08,120
model predicts whatever the ground truth labels are. This is what you want to do. And then what

317
00:28:08,120 --> 00:28:12,920
I just mentioned a couple of times, I mentioned this concept of training. Well this is also when

318
00:28:12,920 --> 00:28:13,300
I'm making a model, I'm going to be making a model that's going to be able to predict the

319
00:28:13,300 --> 00:28:13,320
ground truth labels. So what I'm going to be doing is I'm going to be making a model that's going to

320
00:28:13,320 --> 00:28:13,360
be able to predict the ground truth labels. So what I'm going to be doing is I'm going to be

321
00:28:13,360 --> 00:28:19,680
mentioning training. For the most part what I mean is I mean run your favorite optimization

322
00:28:21,600 --> 00:28:28,720
to find the optimal parameters theta prime such that this model best fits this training set,

323
00:28:28,720 --> 00:28:33,800
right? This is what we want to do. We would like to make sure that our model fits the training set,

324
00:28:33,800 --> 00:28:38,080
explains the labels in the training set. And once we've trained that model, meaning we have tried

325
00:28:38,080 --> 00:28:42,900
to find an optimal solution based on the training set such this model approximates the distance

326
00:28:42,900 --> 00:28:48,600
distribution of these labels here, we hope that our model eventually generalizes. And

327
00:28:48,600 --> 00:28:53,600
generalization means now if I feed in unseen images that we also make reasonable predictions.

328
00:28:55,800 --> 00:29:00,600
Now I mentioned a lot of terms here. I mentioned the concept of training set. I mentioned the

329
00:29:00,600 --> 00:29:06,400
concept of training versus optimization. I mentioned hyperparameters and model parameters.

330
00:29:07,600 --> 00:29:12,500
And I mentioned this concept of generalization. When we like going from a training set where we're

331
00:29:12,500 --> 00:29:17,400
using these ones as constraints and the optimization to train our model versus

332
00:29:17,400 --> 00:29:23,600
predicting and generalizing to unseen observations or to unseen samples, you know, right?

333
00:29:23,600 --> 00:29:29,000
Okay. So if you're trying to go and make this even more abstract, the basic concept of machine

334
00:29:29,000 --> 00:29:37,200
learning follows is mainly data driven. So most, well, pretty much all machine learning methods

335
00:29:37,200 --> 00:29:42,280
come to mind. They all in some way data driven. So when I'm going to talk about machine learning,

336
00:29:42,500 --> 00:29:49,400
I always mean there's some data and based on this data, you're going to train a model. And based on

337
00:29:49,400 --> 00:29:53,900
this model, you're going to make predictions on things that you haven't seen before. Okay. So what

338
00:29:53,900 --> 00:30:01,700
we do first is whenever we develop an algorithm and what we do have in computer vision is for

339
00:30:01,700 --> 00:30:06,500
instance, an image data set, right? Like ImageNet is a great example. That's widely used. So what

340
00:30:06,500 --> 00:30:11,200
we do is we're splitting our data. We're having a train data set. We're having a validation data set

341
00:30:11,200 --> 00:30:11,900
and a test data set.

342
00:30:12,500 --> 00:30:18,700
And what we do here is we say, for instance, we have a split like 60, 20, 20%, right? So this is

343
00:30:18,700 --> 00:30:25,100
100% of our data. We're splitting it up in order to develop our algorithm. And what we would love

344
00:30:25,100 --> 00:30:30,700
to do here is we would like to take our training set. Basically all in the past, what I've told

345
00:30:30,700 --> 00:30:35,700
you in the previous slides is mostly start with the training set. And what we want to do is we

346
00:30:35,700 --> 00:30:42,000
want to find model parameters theta based on the training. What does it mean finding model parameters?

347
00:30:42,500 --> 00:30:47,200
Well, model parameters means we're running an optimization. The training samples are given us

348
00:30:47,200 --> 00:30:52,300
constraints for this optimization. And we're basically running this, this optimization, right?

349
00:30:52,300 --> 00:30:56,700
So based on our distance function, we're trying to find optimal theta prime parameters that give

350
00:30:56,700 --> 00:31:03,000
us good answers. Now, if you're going further, now we have our validation set. You might ask,

351
00:31:03,000 --> 00:31:07,500
well, why on earth do we need a validation and a test set? Can I just train these model parameters

352
00:31:07,500 --> 00:31:11,800
and go make some predictions on having some accuracy or so on my classifier? And I'm going to

353
00:31:11,800 --> 00:31:12,100
be done.

354
00:31:12,500 --> 00:31:18,200
You could do that. However, the problem is assuming you're running your training set,

355
00:31:18,200 --> 00:31:23,600
you're having a bunch of model parameters theta, you want to evaluate how well your model is doing.

356
00:31:23,600 --> 00:31:28,400
So now I'm going to go ahead and going to evaluate my model, how well it's doing. And I'm realizing,

357
00:31:28,400 --> 00:31:32,300
yeah, okay, it's not that great. I would like to change a few things. So for instance,

358
00:31:32,300 --> 00:31:37,100
if I go back quickly, I would like to change this distance function d. I would like to change

359
00:31:37,100 --> 00:31:42,200
the different one. If you did this, this would mean you're changing a hyperparameter here.

360
00:31:42,500 --> 00:31:48,100
And you're training again, and then you're getting a different result, right? So you might say,

361
00:31:48,100 --> 00:31:51,600
well, okay, I'm getting different results when I change some of these hyperparameters.

362
00:31:51,600 --> 00:31:57,500
That's kind of logical. But now you have to re-evaluate again, right? And in this case,

363
00:31:57,500 --> 00:32:01,600
when you're re-evaluating again, you're going to get different results.

364
00:32:01,600 --> 00:32:06,300
So the important thing right now is basically when you are finding hyperparameters,

365
00:32:06,300 --> 00:32:11,800
you want to use the validation set, meaning that you can do this multiple times and you can go ahead and

366
00:32:11,800 --> 00:32:18,200
train, evaluate, change a bunch of parameters, train again, evaluate again, and so on.

367
00:32:18,200 --> 00:32:23,300
So you can do this pretty much as much as you want. And in practice, you would do this as much as you want,

368
00:32:23,300 --> 00:32:28,800
because this is quite important, because you're going to realize some of your models might have certain weaknesses.

369
00:32:28,800 --> 00:32:34,900
Some of your loss functions might not be ideal. Some of your regularizers, we'll talk about this also later,

370
00:32:34,900 --> 00:32:38,200
what that means. Some of your optimization parameters might have been not ideal.

371
00:32:38,200 --> 00:32:41,400
And you might have to change certain things. And if you did this,

372
00:32:41,800 --> 00:32:46,700
that would mean you're looking actually at the results.

373
00:32:46,700 --> 00:32:51,900
So in other words, as soon as you're doing that, you're not looking at the real data anymore,

374
00:32:51,900 --> 00:32:54,900
because as soon as you're considering the validation samples,

375
00:32:54,900 --> 00:33:01,800
you're actually taking those ones as part of your model development, as your model training process.

376
00:33:01,800 --> 00:33:09,800
And because of that, we now need a separate test set that is actually different from the validation set.

377
00:33:09,800 --> 00:33:11,600
And the key critical part here,

378
00:33:11,800 --> 00:33:16,800
is once we found our right hyperparameters, once we're happy with our model choices,

379
00:33:16,800 --> 00:33:21,600
once we're happy with how the model trains and how it evaluates on the validation set,

380
00:33:21,600 --> 00:33:25,600
only then we can go ahead and test on the test set.

381
00:33:25,600 --> 00:33:28,600
And this is really important.

382
00:33:28,600 --> 00:33:32,400
And a lot of people get it wrong, even in the research community.

383
00:33:32,400 --> 00:33:36,600
The important thing is that you only test once.

384
00:33:36,600 --> 00:33:41,600
So as soon as you go ahead and test multiple times, in a sense, you're cheating.

385
00:33:41,800 --> 00:33:47,400
Why are you cheating? Because now you're taking test information from samples where you would like to understand

386
00:33:47,400 --> 00:33:52,800
how well your model channelizes to, you're using that information how to train your model.

387
00:33:52,800 --> 00:33:54,600
And this is really critical.

388
00:33:54,600 --> 00:33:58,400
And I can give you a couple of examples how this could actually work in practice.

389
00:33:58,400 --> 00:34:02,900
So in the research community, typically what happens, there's data sets available.

390
00:34:02,900 --> 00:34:08,400
Computer vision, right? And these data sets, they're going to give you certain results.

391
00:34:08,400 --> 00:34:10,800
So now the problem is, of course,

392
00:34:10,800 --> 00:34:15,000
if you want to objectively evaluate, you want to make sure that,

393
00:34:15,000 --> 00:34:18,600
you know, you have a test set that people don't use over and over again.

394
00:34:18,600 --> 00:34:23,700
Because if people always use their test sets and they're trying so many times while developing the algorithm

395
00:34:23,700 --> 00:34:28,600
and trying all the time on the test set, eventually they're just doing whatever's in the test set, right?

396
00:34:28,600 --> 00:34:31,200
And they will find parameters that give the best test performance.

397
00:34:31,200 --> 00:34:33,900
But they're cheating. They shouldn't have used the test set for that purpose.

398
00:34:33,900 --> 00:34:39,200
In fact, they should have used the validation set and then try to see how well it channelizes to unseen samples

399
00:34:39,200 --> 00:34:40,700
they did not consider doing in their method.

400
00:34:40,800 --> 00:34:46,700
And in practice, what people do then in computer vision often is they have benchmarks.

401
00:34:46,700 --> 00:34:49,300
And benchmark often means that people give out,

402
00:34:49,300 --> 00:34:54,300
for instance, test images and then people have to run the algorithms on the test images

403
00:34:54,300 --> 00:34:58,900
and then they upload their results to an evaluation server.

404
00:34:58,900 --> 00:35:03,100
But the catch is the test image labels are not given to the respective users.

405
00:35:03,100 --> 00:35:04,900
So they don't know what they're doing, basically.

406
00:35:04,900 --> 00:35:08,500
So they're only going to get once a result back from the test image.

407
00:35:08,500 --> 00:35:10,800
Now, you could imagine that people are trying

408
00:35:10,800 --> 00:35:15,700
to get around this. People try to make multiple accounts on these benchmark servers and stuff like this.

409
00:35:15,700 --> 00:35:20,200
But now you can guess, well, this is suddenly a point of cheating

410
00:35:20,200 --> 00:35:25,900
where you're basically not reflecting the channelization to real data anymore.

411
00:35:25,900 --> 00:35:28,300
So this is a very, very important concept.

412
00:35:28,300 --> 00:35:33,500
OK, so bottom line here is use training to optimize the model parameters,

413
00:35:33,500 --> 00:35:38,300
use the validation to find the hyperparameters, iterate as many times as you want.

414
00:35:38,300 --> 00:35:39,900
And until you're happy with that,

415
00:35:39,900 --> 00:35:44,400
then you test at the very end of the day, you test on the test.

416
00:35:44,400 --> 00:35:47,400
Now, if you're going back to our task,

417
00:35:47,400 --> 00:35:50,200
how can we learn now to perform image classification?

418
00:35:50,200 --> 00:35:52,800
Well, we have two things right now. We have a task.

419
00:35:52,800 --> 00:35:55,800
We have image classification. We have experience.

420
00:35:55,800 --> 00:35:58,100
That's our training and validation data.

421
00:35:58,100 --> 00:36:03,200
And now what we do is we have some performance measure that is giving us some accuracy.

422
00:36:03,200 --> 00:36:06,400
And this is typically what we evaluate at the end of the day of the test set.

423
00:36:06,400 --> 00:36:09,700
And then we have a metric that tells us how well does

424
00:36:09,700 --> 00:36:12,500
our method actually work in practice.

425
00:36:12,500 --> 00:36:17,500
Great. So this is the very high level concept of machine learning.

426
00:36:17,500 --> 00:36:20,600
And now if you're asking, well,

427
00:36:20,600 --> 00:36:22,900
why are there so many different machine learning methods?

428
00:36:22,900 --> 00:36:26,100
Well, the question is always how does this model look like

429
00:36:26,100 --> 00:36:29,300
and how do we optimize it and fit it to the respective training data?

430
00:36:29,300 --> 00:36:31,500
How do we make it channelized very well?

431
00:36:31,500 --> 00:36:33,500
And the reason why we're having this lecture

432
00:36:33,500 --> 00:36:39,200
and the reason why it enjoys a lot of popularity right now is that the networks,

433
00:36:39,200 --> 00:36:42,200
neural networks, they perform very well as these models.

434
00:36:42,200 --> 00:36:47,600
And we can show later on that they have great channelization for many, many, many tasks.

435
00:36:47,600 --> 00:36:53,000
OK, so before we get actually into simple machine learning models,

436
00:36:53,000 --> 00:36:57,900
I would like to have a few things that I would like to clarify.

437
00:36:57,900 --> 00:37:05,400
So what I've just described as machine learning for the most part was supervised machine learning.

438
00:37:05,400 --> 00:37:09,000
However, I would also like to acknowledge and I would like to say, well,

439
00:37:09,000 --> 00:37:12,100
there's also unsupervised machine learning.

440
00:37:12,100 --> 00:37:16,600
And I would like to make it a quick differentiation between those

441
00:37:16,600 --> 00:37:20,800
because a lot of people, of course, have heard about these two terminologies

442
00:37:20,800 --> 00:37:23,300
and it's very important to distinguish between those.

443
00:37:23,300 --> 00:37:25,900
So when we're talking about supervised learning,

444
00:37:25,900 --> 00:37:32,000
we always mean we have some labels given, target classes, some regression targets and so on.

445
00:37:32,000 --> 00:37:34,600
And what we'd like to do is we have a training set here.

446
00:37:34,600 --> 00:37:38,900
We have these labels, for instance, here, and we would like to train a model based

447
00:37:39,000 --> 00:37:42,300
on the data such that ideally it matches the distribution

448
00:37:42,300 --> 00:37:46,100
and then channelizes to an unseen set of samples at the end of the day.

449
00:37:46,100 --> 00:37:49,600
Now, here's the challenge.

450
00:37:49,600 --> 00:37:53,800
For the most part of this course, we're going to deal with supervised learning.

451
00:37:53,800 --> 00:37:57,700
There's going to be a few exceptions where we deal with unsupervised learning.

452
00:37:57,700 --> 00:38:01,600
For the most part, I would say 90 plus percent of this course,

453
00:38:01,600 --> 00:38:03,500
we're going to deal with supervised learning.

454
00:38:03,500 --> 00:38:07,700
And when I'm going to talk about machine learning, in the context of this course,

455
00:38:07,700 --> 00:38:08,600
for the most part,

456
00:38:08,600 --> 00:38:10,600
I'm going to refer to supervised learning.

457
00:38:10,600 --> 00:38:12,100
But this is, of course, not true.

458
00:38:12,100 --> 00:38:13,500
There's unsupervised learning.

459
00:38:13,500 --> 00:38:17,700
There's also things like self-supervised learning and so on.

460
00:38:17,700 --> 00:38:19,800
I want to give you a few examples here.

461
00:38:19,800 --> 00:38:23,500
So unsupervised learning is basically when you don't have class labels,

462
00:38:23,500 --> 00:38:27,600
when you don't have any labels, basically.

463
00:38:27,600 --> 00:38:30,900
And the idea of unsupervised learning is, for the most part,

464
00:38:30,900 --> 00:38:34,700
some form of clustering of the structure, right?

465
00:38:34,700 --> 00:38:38,400
So you try to figure out, essentially, so let's say you have the data

466
00:38:38,400 --> 00:38:43,600
and you're trying to divide your samples into clusters

467
00:38:43,600 --> 00:38:46,700
and then you assign a class label to each cluster.

468
00:38:46,700 --> 00:38:49,700
So this class label doesn't necessarily have a semantic meaning,

469
00:38:49,700 --> 00:38:52,900
but it just assigns a class label to it.

470
00:38:52,900 --> 00:38:58,700
And based on the class label, you can then finally, well,

471
00:38:58,700 --> 00:39:01,600
make some use of it in one way or another.

472
00:39:01,600 --> 00:39:03,800
And in that case, what we would do is to say,

473
00:39:03,800 --> 00:39:07,200
okay, here we have, for instance, our example of our cats and dogs again.

474
00:39:07,200 --> 00:39:08,200
Let's say we can cluster them.

475
00:39:08,400 --> 00:39:11,200
Let's say we can cluster those ones into two classes.

476
00:39:11,200 --> 00:39:15,200
Unsupervised learning here would say, well, okay, you have these two classes,

477
00:39:15,200 --> 00:39:17,700
you assign labels, but technically speaking,

478
00:39:17,700 --> 00:39:21,100
you don't know any semantic labels yet unless somebody goes manually

479
00:39:21,100 --> 00:39:25,100
and assigns cluster labels what we've gotten out of it.

480
00:39:25,100 --> 00:39:29,400
There's also a lot of unsupervised learning in the context of deep learning.

481
00:39:29,400 --> 00:39:31,800
It does exist, and we will talk a little bit about it.

482
00:39:31,800 --> 00:39:37,700
But for the most part, the focus of this class will not necessarily,

483
00:39:37,700 --> 00:39:41,000
of course, will not necessarily be on unsupervised learning.

484
00:39:41,000 --> 00:39:46,500
There's also a very fun discussion of what is the more natural thing,

485
00:39:46,500 --> 00:39:50,200
like unsupervised learning versus supervised learning.

486
00:39:50,200 --> 00:39:55,800
It's very philosophical, and researchers in the last decades have gone back and forth.

487
00:39:55,800 --> 00:40:00,100
There's a fun argument, a very philosophical one, is have humans,

488
00:40:00,100 --> 00:40:03,200
do we learn supervised or do we learn unsupervised?

489
00:40:03,200 --> 00:40:06,800
And the fun argument is, well, at the beginning when there was no life on Earth,

490
00:40:06,800 --> 00:40:07,600
eventually,

491
00:40:07,600 --> 00:40:11,500
like humanity, life in general, had to start learning somehow, right?

492
00:40:11,500 --> 00:40:16,500
And then basically the argument is everything can be learned in the unsupervised fashion.

493
00:40:16,500 --> 00:40:19,500
And then the cognitive argument is that is, well, if you have some,

494
00:40:19,500 --> 00:40:23,900
if you don't listen to a language, for instance, as a child,

495
00:40:23,900 --> 00:40:26,000
you're not being able to speak very well.

496
00:40:26,000 --> 00:40:28,500
And you can only do this very well when somebody corrects you,

497
00:40:28,500 --> 00:40:29,900
when somebody tells you certain things.

498
00:40:29,900 --> 00:40:31,600
And a lot of the things your parents do with you

499
00:40:31,600 --> 00:40:34,800
when a kid is actually quite supervised.

500
00:40:34,800 --> 00:40:36,200
And I found this very interesting.

501
00:40:36,200 --> 00:40:37,100
So I'm obviously,

502
00:40:37,100 --> 00:40:43,500
I know I don't understand the biology behind how your brain works here.

503
00:40:43,500 --> 00:40:45,200
But I think it's kind of interesting,

504
00:40:45,200 --> 00:40:48,800
the debate versus like how much supervision do we actually need for learning?

505
00:40:48,800 --> 00:40:50,600
It's a very fun, fun question to ask

506
00:40:50,600 --> 00:40:53,800
when we're talking about modern machine learning techniques,

507
00:40:53,800 --> 00:40:55,800
specifically in the context of representation learning.

508
00:40:55,800 --> 00:40:59,300
A lot of people say, well, you have a large number of labels.

509
00:40:59,300 --> 00:41:01,200
Is there a large number of training samples on the internet,

510
00:41:01,200 --> 00:41:02,600
for instance, where you don't have labels,

511
00:41:02,600 --> 00:41:06,900
but can you still use the data and then combine it later on with some fine tuning?

512
00:41:07,100 --> 00:41:09,200
Where you do actually have labels.

513
00:41:09,200 --> 00:41:12,000
So this is kind of interesting, a lot of cool stuff.

514
00:41:12,000 --> 00:41:16,200
But again, caveat here for the introduction to deep learning here,

515
00:41:16,200 --> 00:41:18,900
we are mostly talking about supervised learning.

516
00:41:18,900 --> 00:41:21,600
I also wanted to mention that there's reinforcement learning.

517
00:41:21,600 --> 00:41:24,900
That's, I guess, kind of in between here.

518
00:41:24,900 --> 00:41:28,700
Reinforcement learning has kind of this interesting concept of agents and environment, right?

519
00:41:28,700 --> 00:41:33,600
So agents interact with an environment and based on their performance,

520
00:41:33,600 --> 00:41:35,700
they're going to get some reward.

521
00:41:35,700 --> 00:41:36,900
And depending on how this reward goes,

522
00:41:37,100 --> 00:41:40,700
your agent is going to adapt and hopefully learn something, right?

523
00:41:40,700 --> 00:41:44,800
So, for instance, in games, board games or even video games,

524
00:41:44,800 --> 00:41:47,900
for instance, reinforcement learning is very popular.

525
00:41:47,900 --> 00:41:50,000
Robotics reinforcement learning is very popular,

526
00:41:50,000 --> 00:41:53,600
where you basically have only a final reward.

527
00:41:53,600 --> 00:41:56,300
And based on that final reward, your model can learn something

528
00:41:56,300 --> 00:41:59,200
and can, for instance, learn certain policies,

529
00:41:59,200 --> 00:42:03,000
like, I don't know, playing chess or learning how to play a video game,

530
00:42:03,000 --> 00:42:07,000
which is kind of cool that you can do this in a kind of,

531
00:42:07,000 --> 00:42:10,900
well, unsupervised fashion.

532
00:42:10,900 --> 00:42:12,800
But I would say this is still a little bit in between

533
00:42:12,800 --> 00:42:14,900
because you still need to define how the reward works.

534
00:42:14,900 --> 00:42:16,700
You still need to define basically when you're winning,

535
00:42:16,700 --> 00:42:18,800
when you're losing, possibly how much you're winning,

536
00:42:18,800 --> 00:42:19,500
how much you're losing.

537
00:42:19,500 --> 00:42:21,500
So it's kind of an interesting question,

538
00:42:21,500 --> 00:42:24,600
how to associate reinforcement learning.

539
00:42:24,600 --> 00:42:28,400
Okay, but let's go back to supervised learning.

540
00:42:28,400 --> 00:42:34,300
I mentioned this is what we would love to talk about for the most part here.

541
00:42:34,300 --> 00:42:37,000
And now,

542
00:42:37,000 --> 00:42:39,800
if we're going back a few slides mentally,

543
00:42:39,800 --> 00:42:43,500
we remember that the whole point of supervised learning was we have our training set

544
00:42:43,500 --> 00:42:47,400
and we would like to figure out a model that we fit to this training set, right?

545
00:42:47,400 --> 00:42:49,700
So again, we have our model M, for instance.

546
00:42:49,700 --> 00:42:55,500
We would like to find parameters theta such that our model approximates

547
00:42:55,500 --> 00:42:58,500
or mimics the distribution of this training set.

548
00:42:58,500 --> 00:43:02,500
Now, the big question is what model do we actually use?

549
00:43:02,500 --> 00:43:06,900
Well, let's start with a simple linear model, for instance, right?

550
00:43:07,000 --> 00:43:13,300
And that's the easiest way of a model we could think about.

551
00:43:13,300 --> 00:43:17,400
So this is a linear decision boundary that we would like to figure out.

552
00:43:17,400 --> 00:43:22,300
So the idea here is we would like to fit a hyperplane

553
00:43:22,300 --> 00:43:26,800
that splits our samples in part here for a classifier.

554
00:43:26,800 --> 00:43:28,500
For regression, it's a little bit different.

555
00:43:28,500 --> 00:43:31,200
We can do a little bit other stuff.

556
00:43:31,200 --> 00:43:33,600
But if you had a classifier, we would like to have a hyperplane.

557
00:43:33,600 --> 00:43:36,400
We would like to split these two things apart.

558
00:43:36,400 --> 00:43:37,700
And this is what we're going to do right now.

559
00:43:37,700 --> 00:43:39,800
So all we're going to do in the next few slides

560
00:43:39,800 --> 00:43:44,300
is we're trying to find parameters for this hyperplane

561
00:43:44,300 --> 00:43:50,300
that figures out how to approximate our training set to the best of our abilities.

562
00:43:50,300 --> 00:43:55,200
And you can already see why I'm saying approximate.

563
00:43:55,200 --> 00:43:57,600
I'm saying this for a very specific reason,

564
00:43:57,600 --> 00:44:02,300
because you can clearly see the common here of a linear model here

565
00:44:02,300 --> 00:44:06,100
is that it cannot possibly

566
00:44:06,100 --> 00:44:07,700
fit all data samples, right?

567
00:44:07,700 --> 00:44:11,400
So if you had one of these triangles, if you had one of them here,

568
00:44:11,400 --> 00:44:14,800
there would be no hyperplane possible anymore to fit in here,

569
00:44:14,800 --> 00:44:17,200
such that it perfectly explains the training set.

570
00:44:17,200 --> 00:44:18,700
And that would be a problem.

571
00:44:18,700 --> 00:44:21,600
So why am I starting with it?

572
00:44:21,600 --> 00:44:24,000
And that's basically the pro. The pro is, well,

573
00:44:24,000 --> 00:44:27,700
it's really simple to understand how basic machine learning works.

574
00:44:27,700 --> 00:44:30,900
And it's probably the simplest model we can begin with.

575
00:44:30,900 --> 00:44:33,400
But it also has certain advantages in practice.

576
00:44:33,400 --> 00:44:35,400
It's actually relatively easy to deploy.

577
00:44:36,100 --> 00:44:37,700
It doesn't take that much, can, right?

578
00:44:37,700 --> 00:44:40,400
It doesn't have so many degrees of freedom.

579
00:44:40,400 --> 00:44:43,900
And it's actually a very nice way to get into it.

580
00:44:43,900 --> 00:44:45,800
So let's start with it.

581
00:44:45,800 --> 00:44:49,200
So now instead of starting with a classifier,

582
00:44:49,200 --> 00:44:52,800
I would like to change the problem statement a little bit.

583
00:44:52,800 --> 00:44:55,700
In this case, we want to start with regression.

584
00:44:55,700 --> 00:44:58,900
And instead of the classifier,

585
00:44:58,900 --> 00:45:00,900
the difference here is when I talk about regression,

586
00:45:00,900 --> 00:45:04,100
it's simply you, instead of predicting a discrete label,

587
00:45:04,100 --> 00:45:06,000
we're now predicting a continuous floating point.

588
00:45:06,000 --> 00:45:07,700
So if we want to do this,

589
00:45:07,700 --> 00:45:11,200
we would like to use this as our output for instance.

590
00:45:11,200 --> 00:45:14,000
And the idea of supervised learning now is, well,

591
00:45:14,000 --> 00:45:17,500
we would like to find a linear model that explains a target Y

592
00:45:17,500 --> 00:45:19,000
given the inputs X.

593
00:45:19,000 --> 00:45:20,500
Well, okay.

594
00:45:20,500 --> 00:45:21,800
So what do we have here?

595
00:45:21,800 --> 00:45:26,400
Well, we're going to start with our training data.

596
00:45:26,400 --> 00:45:29,400
These points here, they are now our training data.

597
00:45:29,400 --> 00:45:30,700
So these red points, right?

598
00:45:30,700 --> 00:45:32,100
They have, they have an X value.

599
00:45:32,100 --> 00:45:33,700
They have a Y value.

600
00:45:33,700 --> 00:45:35,700
So the X values are the inputs.

601
00:45:36,000 --> 00:45:37,740
so

602
00:45:37,740 --> 00:45:43,400
That's what we'd like to approximate. We would like to make sure that this distribution that is given right now for these given inputs

603
00:45:43,400 --> 00:45:46,520
We would like to make sure that if we give importance to this input feature here

604
00:45:48,060 --> 00:45:49,500
Because I shouldn't call it a feature here

605
00:45:49,500 --> 00:45:54,320
I should call it first like a value here for this value here. We like to make the Y value here as a prediction and

606
00:45:55,060 --> 00:45:59,440
The way what linear regression is doing. It's essentially fitting this this little line here

607
00:46:00,120 --> 00:46:01,340
right

608
00:46:01,340 --> 00:46:02,500
and

609
00:46:02,500 --> 00:46:08,820
This fitting this line and I'm mentioning this all the time fitting means which is literally formulating an optimization function here

610
00:46:09,580 --> 00:46:12,560
Is we are optimizing the parameters of this line?

611
00:46:12,580 --> 00:46:18,540
This will become a higher plane obviously later when we go to higher dimensions, but for 2D this is just a simple line

612
00:46:19,420 --> 00:46:20,820
Okay

613
00:46:20,820 --> 00:46:27,420
So what's training mean? Well training means we have a bunch of data points given so data points are these these yellow or orange points

614
00:46:27,420 --> 00:46:28,440
right

615
00:46:28,440 --> 00:46:30,300
These orange points

616
00:46:30,300 --> 00:46:32,300
They are pairs. They have an input

617
00:46:32,840 --> 00:46:37,120
And a target so they have X is the respective like feature. This is the input

618
00:46:38,600 --> 00:46:43,660
This is the measurement could be an image and Y is the respective target label right so

619
00:46:44,380 --> 00:46:45,760
input image

620
00:46:45,760 --> 00:46:47,480
measurement whatsoever and

621
00:46:47,480 --> 00:46:52,740
Y is our respective labels like cat-dog and the classification case or in the regression case

622
00:46:52,740 --> 00:46:59,620
We'll see later a few other examples. It could be could be discrete, right? It could be also continuous. By the way, if you're going to click it back here

623
00:47:00,380 --> 00:47:01,580
um

624
00:47:01,580 --> 00:47:05,440
At the moment, between regression and classification, there's practically no difference.

625
00:47:05,600 --> 00:47:07,620
We'll later see what the differences are.

626
00:47:08,120 --> 00:47:12,380
But the reason for the simplicity right now, let's just assume these labels are continuous.

627
00:47:13,000 --> 00:47:17,040
So if they're classified, we just say, oh, these labels are between 0 and 1, right?

628
00:47:17,380 --> 00:47:21,280
And you're going to make sure that everything between 0.5, then it's label 1.

629
00:47:21,500 --> 00:47:24,100
Everything under 0.5 is going to be label 0, right?

630
00:47:24,320 --> 00:47:26,900
But it could be continuous label 2, right?

631
00:47:27,680 --> 00:47:28,060
Okay.

632
00:47:28,820 --> 00:47:30,200
So we have input measures.

633
00:47:30,200 --> 00:47:31,100
We have labels.

634
00:47:31,580 --> 00:47:40,160
And what we would like to do is we would like to learn the model parameters theta to make the correct estimation.

635
00:47:40,840 --> 00:47:42,080
And what does correct estimation mean?

636
00:47:42,240 --> 00:47:56,100
Well, we would like to make sure that y hat is, if I feed in a new x n plus 1 here, we get a correct estimation y hat.

637
00:47:57,060 --> 00:47:59,120
And you can see what's very important here.

638
00:47:59,120 --> 00:48:00,500
So this notation is important.

639
00:48:00,660 --> 00:48:01,560
We're going to use this a bit.

640
00:48:01,820 --> 00:48:03,500
So x is always the input.

641
00:48:04,060 --> 00:48:05,160
Y is always the target.

642
00:48:05,420 --> 00:48:06,680
So this is our ground truth target.

643
00:48:08,360 --> 00:48:12,260
Y hat is always going to be our predicted estimation.

644
00:48:13,080 --> 00:48:15,760
This is what the model predicts.

645
00:48:16,220 --> 00:48:22,500
X n plus 1, you see n plus 1 here is one index more than what we have in the training set.

646
00:48:22,600 --> 00:48:24,160
This is a test sample here.

647
00:48:24,840 --> 00:48:26,820
So we're feeding this in here in our model.

648
00:48:28,060 --> 00:48:29,760
Theta is our model parameters.

649
00:48:30,020 --> 00:48:31,560
And based on theta, we have.

650
00:48:31,680 --> 00:48:34,400
We're having a predictor that makes this prediction here, right?

651
00:48:35,940 --> 00:48:39,040
And the whole point is that we want to find these parameters theta.

652
00:48:40,260 --> 00:48:42,860
Again, to clarify, this can be the parameters of a neural network.

653
00:48:42,900 --> 00:48:45,400
This concept here does not change right now.

654
00:48:45,740 --> 00:48:51,260
Like the only point right now is going to be whether this theta is part of a linear or nonlinear larger model.

655
00:48:52,340 --> 00:48:55,020
But for the sake of simplicity, we're not quite at neural networks yet.

656
00:48:55,280 --> 00:48:56,740
We're going to make this a little bit easier.

657
00:48:56,740 --> 00:48:59,660
This theta here is now simply a linear model.

658
00:49:00,300 --> 00:49:01,560
And linear model means.

659
00:49:01,860 --> 00:49:03,180
It's linear.

660
00:49:03,460 --> 00:49:04,100
That's the definition.

661
00:49:04,900 --> 00:49:06,480
So what we do here right now is.

662
00:49:07,520 --> 00:49:10,500
We're taking the input here.

663
00:49:10,980 --> 00:49:12,200
So X here is the input.

664
00:49:12,580 --> 00:49:13,880
And we're multiplying with theta.

665
00:49:14,540 --> 00:49:16,880
So imagine this little example that we had here.

666
00:49:17,020 --> 00:49:17,860
I'm going to go quickly back.

667
00:49:18,180 --> 00:49:21,560
In this case, if X is just a single floating point value.

668
00:49:21,920 --> 00:49:23,100
And it's just one scalar.

669
00:49:23,640 --> 00:49:25,540
Then this here.

670
00:49:26,700 --> 00:49:27,800
This D here.

671
00:49:28,320 --> 00:49:29,660
This is the input dimension.

672
00:49:29,920 --> 00:49:30,520
Would be one.

673
00:49:30,640 --> 00:49:31,300
So we just have.

674
00:49:31,300 --> 00:49:33,920
A single theta multiplied with X.

675
00:49:34,620 --> 00:49:35,120
That's it.

676
00:49:35,120 --> 00:49:36,020
And you're predicting Y.

677
00:49:36,020 --> 00:49:42,640
And then you're finding whatever our respective output is going to be approximated best.

678
00:49:43,360 --> 00:49:46,420
Now, in practice, this is a little bit more complicated.

679
00:49:46,520 --> 00:49:47,860
This is the features.

680
00:49:47,900 --> 00:49:49,120
This is why I mentioned this already.

681
00:49:49,120 --> 00:49:50,560
X is not just a single scalar.

682
00:49:50,980 --> 00:49:52,620
X could actually be a vector, right?

683
00:49:52,620 --> 00:49:54,400
There could be many, many, many inputs.

684
00:49:55,060 --> 00:49:57,100
An image is a very large feature, for instance, right?

685
00:49:57,100 --> 00:49:59,980
An image could be like a megapixel, a million pixels, right?

686
00:49:59,980 --> 00:50:00,680
Could be a lot of.

687
00:50:00,680 --> 00:50:01,220
Could be a lot of pixels.

688
00:50:01,220 --> 00:50:01,820
Could be a lot of inputs.

689
00:50:03,440 --> 00:50:05,360
So, and D here is our dimension.

690
00:50:05,840 --> 00:50:06,980
So, oh, I'm not here yet.

691
00:50:06,980 --> 00:50:07,340
Sorry.

692
00:50:07,400 --> 00:50:08,120
These are the weights.

693
00:50:08,120 --> 00:50:10,580
Then these are the model parameters, the theta here.

694
00:50:11,540 --> 00:50:14,160
And D is our input dimension, right?

695
00:50:15,040 --> 00:50:18,920
And again, think about the example, what I just mentioned before.

696
00:50:20,720 --> 00:50:24,900
In this example, if I only have a single dimension, here's input.

697
00:50:24,960 --> 00:50:26,220
I just have an X here.

698
00:50:27,020 --> 00:50:28,580
And I want to fit a plane to it.

699
00:50:30,440 --> 00:50:30,980
Oh, in one day.

700
00:50:30,980 --> 00:50:31,160
Actually.

701
00:50:31,220 --> 00:50:32,420
I want to just fit in this line.

702
00:50:33,220 --> 00:50:36,200
Now I'm cheating a little bit here.

703
00:50:36,200 --> 00:50:38,360
This is technically a little bit wrong for my formulation.

704
00:50:39,140 --> 00:50:41,500
If you go into the 1D case, you will quickly notice this.

705
00:50:41,540 --> 00:50:43,040
You also need a bias here.

706
00:50:43,580 --> 00:50:46,700
So you need, um, you need a theta zero.

707
00:50:47,220 --> 00:50:49,160
Um, why do you need that?

708
00:50:49,180 --> 00:50:52,760
Well, technically you don't need it, but I think it makes the fitting a little bit easier.

709
00:50:53,340 --> 00:50:59,300
So technically you need, um, where the, uh, line here intersects with the Y axis, right?

710
00:50:59,780 --> 00:51:01,200
Um, and this one is important.

711
00:51:01,380 --> 00:51:03,860
Because otherwise you cannot define arbitrary lines.

712
00:51:03,860 --> 00:51:11,240
Otherwise they would only go, uh, through zero, zero, and you would just literally define the slope here, right?

713
00:51:11,240 --> 00:51:18,980
But we have the slope and we have, um, uh, we have basically where, uh, the line intersects with the Y axis here.

714
00:51:19,720 --> 00:51:20,220
Okay.

715
00:51:20,420 --> 00:51:22,040
And, but then the rest is the same, right?

716
00:51:22,040 --> 00:51:24,900
So we have here the bias plus the formula we just had, right?

717
00:51:24,900 --> 00:51:29,220
We just say we multiply X, I, J with theta chain, right?

718
00:51:29,340 --> 00:51:31,220
Input feature dimension plus with the kernel.

719
00:51:31,220 --> 00:51:40,100
And then we just multiply it out, and this is essentially what we're getting from our sum here, right?

720
00:51:40,960 --> 00:51:41,320
Okay.

721
00:51:42,280 --> 00:51:45,400
Um, I think let's have an example, right?

722
00:51:45,480 --> 00:51:47,100
This is actually relatively easy.

723
00:51:47,240 --> 00:51:50,060
So this shouldn't be something new to you from a mathematical perspective.

724
00:51:50,260 --> 00:51:53,520
So all we're trying to have is we have a certain set of constraints.

725
00:51:53,520 --> 00:51:56,880
Our constraints are given by our data points, these orange points here.

726
00:51:58,180 --> 00:52:00,920
Um, and what we would like to do is.

727
00:52:01,220 --> 00:52:12,580
We would like to make sure that the thetas are optimized such that these points here are approximated in the best possible way, best possible way is also something we'll define in a second, because depending on our loss function.

728
00:52:13,380 --> 00:52:13,880
Okay.

729
00:52:13,940 --> 00:52:15,140
Let's make a linear prediction.

730
00:52:16,220 --> 00:52:18,560
Let's predict the temperature of a room.

731
00:52:18,620 --> 00:52:27,740
Let's assume we want to make sure, um, I'm going to give you a bunch of information and I would like to ask you, what is the temperature of a building?

732
00:52:28,700 --> 00:52:30,500
Well, what information could I give you?

733
00:52:30,800 --> 00:52:31,200
And this is.

734
00:52:31,260 --> 00:52:32,720
The features that I'm giving you right now.

735
00:52:33,260 --> 00:52:36,200
So I'm going to give you four different type of features here.

736
00:52:36,380 --> 00:52:38,120
I'm going to tell you how warm it is outside.

737
00:52:39,500 --> 00:52:41,180
I'm going to tell you the level of humidity.

738
00:52:41,780 --> 00:52:47,360
I'm going to tell you the number of people in the building, and I'm going to tell you the sun exposure to the building, right?

739
00:52:48,080 --> 00:52:48,500
Okay.

740
00:52:48,740 --> 00:53:00,020
And now what I would like to do is I would like to, I would like to ask you based on these inputs that you have, what is the most likely temperature of the inside of the building?

741
00:53:00,040 --> 00:53:01,220
So give me a model.

742
00:53:01,520 --> 00:53:08,120
The dust that, um, and in order to do that, what I'm, what I'm, what I'm giving you also is I'm going to give you a bunch of supervision.

743
00:53:08,120 --> 00:53:15,980
So I'm going to give you a bunch of ground truth pairs for training, where I pair these four dimensional input features here with our respective prediction.

744
00:53:16,220 --> 00:53:16,460
Right.

745
00:53:17,040 --> 00:53:17,460
Okay.

746
00:53:17,640 --> 00:53:18,460
And that's what we're going to do.

747
00:53:18,740 --> 00:53:24,200
Um, so this is a trick question now, how many parameters do we need to fit that?

748
00:53:24,660 --> 00:53:30,840
Well, we're taking this formula that we had before each of these features is associated from data, right?

749
00:53:30,840 --> 00:53:39,520
So all we're doing right now is just rebating how each of these features contribute to the input or to, to the output temperature of the building.

750
00:53:40,480 --> 00:53:40,840
Right.

751
00:53:41,420 --> 00:53:43,740
Um, and we have a bias, of course, so we have one more.

752
00:53:44,480 --> 00:53:49,360
Um, so we have this, um, and now what we can do is we can write this down as a matrix form, right?

753
00:53:50,280 --> 00:53:57,700
Uh, so now what we do is we just hit, um, here, we're going to have the respective prediction of our model here.

754
00:53:57,700 --> 00:53:59,380
We're going to have the respective thetas.

755
00:54:00,840 --> 00:54:06,780
Um, and here we're going to have the respective, um, input features that we have.

756
00:54:06,900 --> 00:54:09,780
So this D here in our case, this one was four, right?

757
00:54:10,480 --> 00:54:12,300
Um, so this one went from one to four.

758
00:54:13,200 --> 00:54:30,560
Um, and this N here is how many measures we have, like the number of rows in this equation tells us how many samples we have and the number of, sorry, the number of columns tells us, um, how many features we have and the number of rows.

759
00:54:30,840 --> 00:54:33,900
Um, tells us how many samples we have, right?

760
00:54:34,460 --> 00:54:34,900
Okay.

761
00:54:35,220 --> 00:54:40,140
And the idea is that if we, now we can pull this, this bias actually in here, right?

762
00:54:40,160 --> 00:54:42,720
So we just having, we're just combining this matrix here.

763
00:54:42,720 --> 00:54:45,400
So we're just adding, um, another column with ones here.

764
00:54:45,420 --> 00:54:49,680
So we end with theta zero here now, uh, and we can just simply write this as a matrix form.

765
00:54:49,680 --> 00:54:53,160
So now what we know is Y hat is equal to X times theta.

766
00:54:54,220 --> 00:54:56,080
And now it's important what we have here.

767
00:54:56,280 --> 00:55:00,160
Y is a vector, X is a matrix and theta is also a vector.

768
00:55:00,840 --> 00:55:05,620
Theta is in fact our unknown vector, which is the most important thing we'd want to figure out this one right now.

769
00:55:06,320 --> 00:55:09,100
Um, well, that's what we'd like to figure out.

770
00:55:09,940 --> 00:55:10,340
Okay.

771
00:55:11,140 --> 00:55:15,360
Um, so this is a linear equation system, right?

772
00:55:15,360 --> 00:55:20,400
Um, what you would know from, from, um, analysis and algebra.

773
00:55:21,200 --> 00:55:30,820
Um, so linear algebra tells you that depending on how many unknowns you have, you need a certain number of equations.

774
00:55:30,840 --> 00:55:31,340
Right?

775
00:55:31,340 --> 00:55:39,940
So what might happen here is your equations might, so your system here might be overdetermined, undetermined, or fully determined, right?

776
00:55:39,940 --> 00:55:49,020
So if the thetas and the number of samples are the same, this is going to be a, uh, this here is going to be a square matrix, right?

777
00:55:49,020 --> 00:55:59,820
Um, sorry, this here is also, this here is going to be a square matrix too, of course then, um, and then we can, can have a unique solution where we have the best fit possible.

778
00:55:59,820 --> 00:56:00,320
If we have two.

779
00:56:00,320 --> 00:56:00,820
Okay.

780
00:56:01,320 --> 00:56:02,340
More of them.

781
00:56:03,340 --> 00:56:06,980
Well, then we have to find a different way, or if you have a few of them, we also have to find a different way.

782
00:56:07,440 --> 00:56:10,560
And little spoiler, what I'm practically going to do is we're going to do a least squares fit.

783
00:56:11,520 --> 00:56:12,300
Um, okay.

784
00:56:12,660 --> 00:56:14,020
So let's make this more concrete again.

785
00:56:14,040 --> 00:56:22,860
I want to, I want to make sure that everybody gets this, because this is very fundamental for what we're going to do with the neural networks, because neural networks are not so far away from this, what we're doing here right now.

786
00:56:23,520 --> 00:56:25,900
So in fact, this here, we have the predictions of our model.

787
00:56:26,220 --> 00:56:28,160
So the y hat one to y hat n.

788
00:56:28,920 --> 00:56:30,820
Uh, we're going to have the model parameters.

789
00:56:31,380 --> 00:56:34,740
Which we have here, theta zero to theta d.

790
00:56:35,640 --> 00:56:36,780
This is the dimension here.

791
00:56:37,600 --> 00:56:42,460
Um, we also know, um, the input feature dimensionality, right?

792
00:56:42,480 --> 00:56:48,000
Obviously what feature dimensionality plus one needs to match with the number of, um, parameters in our model.

793
00:56:48,020 --> 00:56:51,940
Otherwise this like a row times calling multiplication doesn't work anymore.

794
00:56:52,600 --> 00:57:00,320
Um, and yeah, and then what we need to know now is, well, we have these input features one.

795
00:57:00,840 --> 00:57:08,100
Sample here has D features or D dimensional feature, and you could call the whole thing one feature as well.

796
00:57:08,120 --> 00:57:09,420
And it's a feature vector then.

797
00:57:10,060 --> 00:57:13,160
Um, and then we have our model respectively.

798
00:57:13,840 --> 00:57:14,340
Okay.

799
00:57:14,660 --> 00:57:17,040
So let's make it more concrete in our example.

800
00:57:17,460 --> 00:57:20,160
So our example was our temperature in the building.

801
00:57:20,640 --> 00:57:21,780
So what do we have here?

802
00:57:22,200 --> 00:57:25,060
We have the, the T this one was the bias at the beginning.

803
00:57:25,780 --> 00:57:27,540
Um, we have the outside temperature.

804
00:57:27,580 --> 00:57:30,820
We have humidity, number of people and sun exposure.

805
00:57:30,860 --> 00:57:32,920
That's seven percent, right?

806
00:57:33,540 --> 00:57:40,280
Um, so now what we want to do is we want to figure out how can we train our model?

807
00:57:40,300 --> 00:57:45,880
Well, our model right now has 1, 2, 3, 4, 5 parameters.

808
00:57:45,880 --> 00:57:50,300
So for these parameters are directly associated with the respective input features.

809
00:57:51,400 --> 00:57:53,520
And one of them is the bias.

810
00:57:54,600 --> 00:57:59,160
So if, for instance, one of these.

811
00:58:00,840 --> 00:58:05,560
entries here zero that means this info feature will be ignored right if there's a zero then this

812
00:58:05,560 --> 00:58:10,120
feature will not contribute to the output predictions um if this one has a very high

813
00:58:10,120 --> 00:58:15,720
value then this feature will contribute a lot and what's also interesting is assuming for instance

814
00:58:15,720 --> 00:58:20,040
the sun exposure here is in percent assuming i didn't write the percentage in integers and i

815
00:58:20,040 --> 00:58:25,640
would divide it by 100 well ideally our model would figure out um that you just have to divide

816
00:58:25,640 --> 00:58:29,480
the model parameters here also by 100 right so otherwise you would actually change the result

817
00:58:29,480 --> 00:58:33,400
so in principle the scaling and stuff like this should factor out because if you multiply this

818
00:58:33,400 --> 00:58:39,640
one you should just divide this one respectively right and so on okay but it is very straightforward

819
00:58:39,640 --> 00:58:46,040
right so we have here our uh our our our made up samples that we have so we have two samples here

820
00:58:46,040 --> 00:58:51,400
that we are making up uh these are our made up things um if you're having two samples we

821
00:58:52,200 --> 00:58:57,400
uh know already that we are under constraint right so we have two predictions for these two samples

822
00:58:59,080 --> 00:58:59,400
um

823
00:58:59,480 --> 00:59:04,120
but we have our model so if we have the model already what can we do here for this model we

824
00:59:04,120 --> 00:59:09,000
can make certain predictions so if we have these input features and these input features our model

825
00:59:09,000 --> 00:59:13,480
will make certain predictions here right so if you're running this through the model this one

826
00:59:13,480 --> 00:59:20,360
would mean oh this one uh is great this is warm and this one would mean oh no it's cold

827
00:59:20,360 --> 00:59:23,720
i didn't do the multiplication here but i assume this is a larger number and this one

828
00:59:23,720 --> 00:59:29,400
i assume is a smaller number okay um but now the big question is so this is a

829
00:59:29,480 --> 00:59:30,440
so this is a larger number okay so this is a larger number okay so let's assume we have these model

830
00:59:30,440 --> 00:59:34,200
let's assume we have these model parameters here but now the question in the learning process is

831
00:59:34,200 --> 00:59:38,680
how do we actually obtain these model parameters and i mentioned it a couple of times this leads to

832
00:59:40,440 --> 00:59:45,000
linearly squares optimization so we have to figure out how can we find these parameters

833
00:59:45,000 --> 00:59:50,200
how can we solve these equation systems okay how do we do it well we have the data points x here

834
00:59:51,000 --> 00:59:55,160
uh these data points x they give you give us our model parameters

835
00:59:55,160 --> 00:59:59,480
so they're being fed into the model parameters based on these ones we're making an estimation

836
01:00:00,120 --> 01:00:05,320
um we have here our ground truth we're defining a loss function here and based on that loss

837
01:00:05,320 --> 01:00:11,880
function we're updating the model parameters again right and then we optimize right um and that's

838
01:00:11,880 --> 01:00:18,040
what we're going to do data points is input model parameters estimation loss function how wrong are

839
01:00:18,040 --> 01:00:23,640
we and then we optimize okay so what are the two important things here we have introduced two two

840
01:00:23,640 --> 01:00:29,400
important concepts here so the concept here is we have the loss function that measures the loss of

841
01:00:29,480 --> 01:00:37,160
how good is my estimate and tell the optimization how to make it better right so how good's my model

842
01:00:37,880 --> 01:00:42,600
and how do i make it better like what is how to make it better mean well the idea for the loss

843
01:00:42,600 --> 01:00:47,400
function is it should get smaller and the model gets better right if the loss function is high or

844
01:00:47,400 --> 01:00:52,520
if the loss is high um then we know our model is not that great and we have to change something so

845
01:00:52,520 --> 01:00:58,200
if i change something my loss goes down but i'm like yeah i've been great and the optimization it

846
01:00:58,200 --> 01:01:03,000
changed the model in order to improve the loss function right so i want to improve the estimations

847
01:01:03,000 --> 01:01:09,400
based on my training samples that i have available and again this concept here is the same across all

848
01:01:09,400 --> 01:01:14,280
of machine learning very much okay maybe there's certain things that do slightly differently but

849
01:01:14,280 --> 01:01:19,240
this high level concept is we have a loss function we want to minimize it we want to optimize it and

850
01:01:19,240 --> 01:01:24,680
based on the optimization we want to change the model in order to improve the loss function right

851
01:01:24,680 --> 01:01:28,040
so how do we do with this in the context of the linear regression well again i'm

852
01:01:28,040 --> 01:01:33,160
reiterating here we have to solve our linear uh least square system so we have this prediction

853
01:01:33,160 --> 01:01:39,320
of the temperature in the building um and this optimization process pretty much goes like that

854
01:01:39,320 --> 01:01:42,520
right so what i'm going to do is i'm going to going to have this line here i'm going to check

855
01:01:42,520 --> 01:01:49,000
my distance i'm going to see huh all right this gives me a certain value uh now i have another

856
01:01:49,000 --> 01:01:54,280
value that is up there and if the value is up there and tells me oh no there's a high loss here

857
01:01:54,280 --> 01:01:54,680
i have to

858
01:01:54,680 --> 01:02:00,920
to change my line right so i have to update i have to update this line such that the overall

859
01:02:00,920 --> 01:02:06,120
average loss actually goes down this is what this loss function is going to do right and i'm trying

860
01:02:06,120 --> 01:02:10,520
to use an average and treating all samples equally right now we'll later talk about what we do with

861
01:02:10,520 --> 01:02:15,720
these outliers because that's obviously a bad case but this is what we would like to address okay

862
01:02:16,840 --> 01:02:23,240
um well what we do okay so we mentioned we have to solve our famous linear least squares problem

863
01:02:23,960 --> 01:02:24,280
right

864
01:02:24,280 --> 01:02:30,600
so what we do is we just take an l2 loss um we compare our predictions with the respective

865
01:02:30,600 --> 01:02:38,440
ground truth samples um and we're trying to minimize that that that function right so

866
01:02:38,440 --> 01:02:43,720
that's all we're going to do so also what you'll see a lot of people will call this function very

867
01:02:43,720 --> 01:02:49,080
differently some people call it objective objective function energy cost function and so on

868
01:02:49,080 --> 01:02:53,720
all the same stuff right just depending who you're gonna ask if it's a math guy a machine learning guy

869
01:02:53,720 --> 01:02:54,200
and so on

870
01:02:55,080 --> 01:02:58,840
okay so how do we do that well we have to optimize for that

871
01:02:59,800 --> 01:03:05,720
um and now we have to minimize that function so minimization here means minimize

872
01:03:06,760 --> 01:03:11,960
this function here in order to fit the linear model to the respective data points that's what

873
01:03:11,960 --> 01:03:16,680
we care about right now right okay this is actually a convex problem

874
01:03:17,560 --> 01:03:23,000
meaning that there is actually a closed form solution solution that is unique um and of course

875
01:03:23,640 --> 01:03:30,600
linear algebra tells you exactly how to do that um and if you're doing this the way this is done

876
01:03:30,600 --> 01:03:34,280
is we're doing this over all the training samples so we have n training samples here

877
01:03:34,280 --> 01:03:40,600
so we're taking the average so we'll be dividing by n here um and the important

878
01:03:40,600 --> 01:03:48,520
thing right now here is that this y hat we have to express actually with our respective model

879
01:03:48,520 --> 01:03:52,520
all right so our respective model like you might be surprised so why on earth what is he talking

880
01:03:52,520 --> 01:03:59,240
about like there's a theta here um but there's no theta here anymore okay um so this y hat is

881
01:03:59,240 --> 01:04:05,800
nothing else but xi times theta uh this is the estimation from the linear model this is the

882
01:04:05,800 --> 01:04:11,320
linear model what it's doing uh and note that this is extremely matrix notation here right already

883
01:04:11,320 --> 01:04:16,280
right so this one here is going over the training samples but this one here is one feature vector

884
01:04:16,280 --> 01:04:22,440
times our vector of weights and this is a a vector this is a dot product here right

885
01:04:22,440 --> 01:04:28,320
and this dot product because there's another sum here we can now go ahead and write write the whole

886
01:04:28,320 --> 01:04:34,840
thing as a matrix notation right so we can write this square thing as the matrix notation um in

887
01:04:34,840 --> 01:04:41,840
this case we know this one is now a matrix this is n training samples each has an input vector of

888
01:04:41,840 --> 01:04:46,280
of d and this x here is written as a matrix basically all right so these

889
01:04:46,280 --> 01:04:52,060
are our training samples here and here um and of course i'm assuming you know also this function

890
01:04:52,060 --> 01:04:58,300
this is nothing else but the normal equation it's the same thing um and we also know that this y here

891
01:04:58,300 --> 01:05:03,120
is our n label that we care about right so these ones we want to approximate this is a matrix this

892
01:05:03,120 --> 01:05:09,560
is a vector this is also a vector right um we will talk a little bit about the matrix notation in the

893
01:05:09,560 --> 01:05:15,300
exercise session um we understand a lot of people taking this lecture um well let's put it this way

894
01:05:15,300 --> 01:05:19,600
the linear algebra course might have been a few semesters ago and we're happy to give a bit of a

895
01:05:19,600 --> 01:05:24,980
refresher course here so you want to keep this course relatively easy to follow so we are actually

896
01:05:24,980 --> 01:05:30,100
giving you um a bit of an intro on the matrix notation so if you were a bit struggling here

897
01:05:30,100 --> 01:05:37,980
i would recommend maybe pause the video here and actually go and and check our our tutorial on the

898
01:05:37,980 --> 01:05:42,700
matrix notation first um however at the same time for the rest of the sake of the video i'm assuming

899
01:05:42,700 --> 01:05:44,400
that you know or at least have a basic

900
01:05:44,400 --> 01:05:45,280
knowledge of the matrix notation so i'm assuming that you know or at least have a basic knowledge

901
01:05:45,280 --> 01:05:51,340
of matrix notations okay all right so what does it mean well this is obviously a convex function

902
01:05:51,340 --> 01:05:56,720
um this is the normal equation right um we know that from linear algebra um and we know that

903
01:05:56,720 --> 01:06:01,380
there's an optimum and we know how to find the optimum and the way how we do find that optimum

904
01:06:01,380 --> 01:06:06,900
is very straightforward um actually we have learned this already in high school in high

905
01:06:06,900 --> 01:06:10,860
school we know if we have a quadratic function like this one this is a quadratic function it's

906
01:06:10,860 --> 01:06:15,080
the normal equation um then we know that the gradient um

907
01:06:15,280 --> 01:06:19,700
when the gradient is zero here we know that we have found a minimum or a maximum depending on

908
01:06:19,700 --> 01:06:24,800
what shape the function is right but we have an extrema when the gradient is going to be zero

909
01:06:24,800 --> 01:06:33,020
so how do we optimize that well um it's also relatively straightforward we just compute the

910
01:06:33,020 --> 01:06:38,120
gradient of this matrix form right and if you're taking the gradient with respect to the thetas

911
01:06:38,120 --> 01:06:44,060
that's what we care about these are our parameters we would like to solve for we end up with two times

912
01:06:45,280 --> 01:06:51,540
x times theta minus two times x transpose times y and this should be equal to zero right this is

913
01:06:51,540 --> 01:06:57,000
all constrained here so we're saying please find me the gradient with a finding point with the

914
01:06:57,000 --> 01:07:03,080
gradient zero that's what we're setting here and the way we're finding that is it's xt times x

915
01:07:03,080 --> 01:07:10,240
inverse times xty right and again you will notice this very rightfully this is this is the normal

916
01:07:10,240 --> 01:07:15,260
equation right like if you if you bring this one here to the other side then you have xt times

917
01:07:15,280 --> 01:07:22,780
x times theta is equal to xt times y which is our residual this is nothing else but a linear system

918
01:07:22,780 --> 01:07:29,200
to solve right um there's a lot of a lot of things um i would love to give maybe a bit of

919
01:07:29,200 --> 01:07:35,460
more intro on the algebra um this is the thing that i i unfortunately still experience even at

920
01:07:35,460 --> 01:07:39,680
the phd level that people often struggling is how to solve systems like these ones right

921
01:07:39,680 --> 01:07:45,260
um this is something that is very important like how do you even remember a matrix like this right

922
01:07:45,280 --> 01:07:49,180
so in practice once you would never invert a matrix like this you would always use a linear

923
01:07:49,180 --> 01:07:55,420
solver library especially when the matrix becomes large um there will be details in the exercise

924
01:07:55,420 --> 01:08:00,300
session so again please look into this when you have any struggling with that i know there's a

925
01:08:00,300 --> 01:08:04,680
bit of redundancy that's why i moved a little bit of stuff out here um to the exercise sessions and

926
01:08:04,680 --> 01:08:09,900
i hope that you can catch up with the things you might have forgotten about linear algebra

927
01:08:09,900 --> 01:08:15,020
okay um but assuming we can do that right we have found an

928
01:08:15,280 --> 01:08:21,940
easy solution to a complex problem um what do we do here again well the inputs here are

929
01:08:21,940 --> 01:08:28,840
our features like outside temperature number of people humidity um and on and this here on the

930
01:08:28,840 --> 01:08:34,180
right hand side is our true output it's the temperature of the building and this is a least

931
01:08:34,180 --> 01:08:43,460
squares estimate right um so there's this is not necessarily the best estimate but this is the

932
01:08:43,460 --> 01:08:45,260
simplest estimate this is why we started

933
01:08:45,280 --> 01:08:51,760
with this one um and there's a couple of different estimators here we're going to go through

934
01:08:51,760 --> 01:08:57,600
um and each estimator has different properties right so for instance one thing what we have seen

935
01:08:57,600 --> 01:09:04,640
here if you're taking a square term here you will see that if we're having one outlier in this least

936
01:09:04,640 --> 01:09:08,820
squared estimate we will unfortunately have a bit of a problem because if we have one outlier

937
01:09:08,820 --> 01:09:14,540
then unfortunately um yeah we might not be very robust to these outliers so for instance if you're

938
01:09:14,540 --> 01:09:15,260
taking an l1

939
01:09:15,280 --> 01:09:21,360
loss here instead of an l2 loss um then we might be a little bit more robust to outliers

940
01:09:21,360 --> 01:09:26,960
unfortunately optimization becomes a little bit more tricky right so again also i hope that most

941
01:09:26,960 --> 01:09:31,440
people know that i'm still going to repeat it a couple of times right if you're having an l2 loss

942
01:09:31,440 --> 01:09:35,680
here it's a least squares estimate the optimum will always be the mean and if you have one

943
01:09:35,680 --> 01:09:41,200
outlier the mean will shift quite a bit if you have an l1 loss for instance here the optimum

944
01:09:41,200 --> 01:09:45,120
here will be the median and then if you have one outlier that will not affect the result that much

945
01:09:45,280 --> 01:09:52,400
okay all right so let's talk a little bit more so this is basically an estimate we have done

946
01:09:52,400 --> 01:09:59,420
based on an least square estimate now what i would like to do is i would like to talk about this

947
01:09:59,420 --> 01:10:05,420
from a probabilistic standpoint and and this is a thing called maximum likelihood so it's very much

948
01:10:05,420 --> 01:10:12,100
related um and we want to show why it's related um to our linear regression that we have just seen

949
01:10:12,800 --> 01:10:15,080
so maximum likelihood is also a concept

950
01:10:15,280 --> 01:10:21,600
you might have heard in your stochastic lectures already um but the core idea here is very specific

951
01:10:21,600 --> 01:10:27,040
to machine learning meaning that what we would love to do is we would like to love to look at

952
01:10:27,040 --> 01:10:34,720
some data um and this data has a certain distribution right so it might be a sequence of

953
01:10:34,720 --> 01:10:41,280
numbers it might be a bunch of images it might be language right there's a true distribution right

954
01:10:42,000 --> 01:10:44,000
um so given samples

955
01:10:44,640 --> 01:10:45,280
this is basically

956
01:10:45,280 --> 01:10:51,440
our true distribution here this is our data what we have right so we have here our uh what is our

957
01:10:51,440 --> 01:10:55,680
respective value conditioned on the respective input right that's what we care about this is

958
01:10:55,680 --> 01:11:02,720
what's in our data that's our true distribution that we would care about um and then what we have

959
01:11:02,720 --> 01:11:08,400
is we have our model now our model we would like to mimic our data to some degree and i'm going

960
01:11:08,400 --> 01:11:14,640
to talk about what this means by mimic it um so let's assume our model here is a parametric family

961
01:11:14,640 --> 01:11:22,080
of distributions right so how is it parameterized well this is these these data parameters that's

962
01:11:22,080 --> 01:11:26,320
what we've been talking about all the time right now right so these data parameters they are model

963
01:11:26,320 --> 01:11:33,120
parameters what we would like to do is that these theta parameters make sure that our model right

964
01:11:33,120 --> 01:11:43,120
now here um approximates basically our respective distribution and what a maximum likelihood

965
01:11:43,120 --> 01:11:44,480
estimate is now saying is that

966
01:11:45,440 --> 01:11:51,600
it's a method of estimating the parameters of a statistical model given the distributions

967
01:11:52,800 --> 01:11:59,040
uh sorry given the observations so what we have we have here our model we were just defined what

968
01:11:59,040 --> 01:12:04,640
we care about is the theta these are our model parameters um and now we have a bunch of

969
01:12:04,640 --> 01:12:12,880
observations from our data right and what we would like to do now is we would like to find the parameters

970
01:12:14,880 --> 01:12:21,040
that maximize the likelihood of making the observations given the parameters right so in

971
01:12:21,040 --> 01:12:25,680
other words we want to make sure the model predicts what our observations say right that's

972
01:12:25,680 --> 01:12:31,920
basically what it says so we would like to make sure that we're finding these parameters and in

973
01:12:31,920 --> 01:12:37,600
a mathematical sense what that means is we would like to make sure we maximize the likelihood

974
01:12:37,600 --> 01:12:44,000
that's why it's called maximize likelihood that the model matches whatever the true observations

975
01:12:44,000 --> 01:12:44,480
are going to say

976
01:12:44,800 --> 01:12:50,640
and since we're assuming right now this is a stochastic process this is why it's an estimate

977
01:12:50,640 --> 01:12:57,040
and it's a likelihood estimate okay um now there's a couple of assumptions what we're making here

978
01:12:57,040 --> 01:13:00,400
um and there's basically two two main assumptions

979
01:13:01,520 --> 01:13:08,160
um so what we're assuming is that the maximum likelihood estimate assumes that the training

980
01:13:08,160 --> 01:13:13,840
samples are independent and they're generated all by the same probability distribution we'll later

981
01:13:13,840 --> 01:13:14,640
talk about which distribution is independent and they're generated all by the same probability

982
01:13:14,640 --> 01:13:19,080
distribution it is but they're independent and they're from the same distribution and that's

983
01:13:19,080 --> 01:13:22,480
important this is our assumptions what we're making with our maximum likelihood estimate

984
01:13:22,480 --> 01:13:28,840
and assuming we say these are independent and this is very important now because assuming they're

985
01:13:28,840 --> 01:13:35,800
independent for the sampling strategy this means all we have to do is we check our model we make

986
01:13:35,800 --> 01:13:41,600
a prediction here basically every time that means we can simply multiply all of these predictions

987
01:13:41,600 --> 01:13:47,080
from our model in other words none of the predictions have anything to do with each other

988
01:13:47,080 --> 01:13:51,820
from a probabilistic standpoint right that's why we're assuming the samples that we're drawing from

989
01:13:51,820 --> 01:13:56,540
our model right now they're independent and they're generated by the same probability distribution

990
01:13:56,540 --> 01:14:04,480
okay so this is what we're assuming and now what we would like to do is we would like to go from

991
01:14:04,480 --> 01:14:11,500
this abstract notion of please maximize the likelihood that my model matches

992
01:14:11,500 --> 01:14:11,580
my model and that's what we're assuming and now what we would like to do is we would like to go

993
01:14:11,580 --> 01:14:12,120
from this abstract notion of please maximize the likelihood that my model matches my observations

994
01:14:12,120 --> 01:14:19,360
i would like to derive a loss function i would like to figure out how can we go from this

995
01:14:19,360 --> 01:14:25,760
function here or from from this kind of abstract maximum likelihood estimate how can we go to a

996
01:14:25,760 --> 01:14:33,460
loss function that we can then actually optimize in order to find the optimal parameters such that

997
01:14:33,460 --> 01:14:39,500
our model is actually having the right status in order to match the observations with the highest

998
01:14:39,500 --> 01:14:41,220
probability okay

999
01:14:41,580 --> 01:14:49,800
so how do we do this well we want to simplify this term a little bit here and the big problem

1000
01:14:49,800 --> 01:14:54,100
what we're going to have here well let's call it a problem for now but we will find a solution

1001
01:14:54,100 --> 01:15:00,520
obviously um is this product this product makes this art max here kind of nasty to optimize when

1002
01:15:00,520 --> 01:15:04,680
if you're thinking about how you would optimize that it's not that straightforward but there's a

1003
01:15:04,680 --> 01:15:10,020
simple simple thing what we can do um so what we're going to do right now is we're going to go ahead

1004
01:15:10,020 --> 01:15:11,420
and just drawing the

1005
01:15:11,580 --> 01:15:17,780
logarithm so well looks strange right so what we're doing is we are basically going ahead and

1006
01:15:17,780 --> 01:15:25,100
saying here we have a product and here we have a sum but we have a sum of logarithms and this is

1007
01:15:25,100 --> 01:15:32,800
the nice thing about the the logarithmic property um if we're having a log a times b it's the same

1008
01:15:32,800 --> 01:15:41,500
as log a plus log b and you might say well okay that's fair so if you drew the logarithm

1009
01:15:41,580 --> 01:15:48,320
here um then you could do that but now you introduce just randomly logarithm in order

1010
01:15:48,320 --> 01:15:54,980
then to make this product here a sum but the trick here is that this is the logarithm is a

1011
01:15:54,980 --> 01:16:02,840
monotonic function so if we took the logarithm versus not the logarithm like the the monotony

1012
01:16:02,840 --> 01:16:07,560
is not violated because we we still have the highest probability of making the right prediction

1013
01:16:07,560 --> 01:16:10,300
or like to make the right prediction of the respective observations

1014
01:16:11,580 --> 01:16:16,680
this is a monotonic function we can very easily just draw the logarithm here and then rewrite this

1015
01:16:16,680 --> 01:16:21,520
product here as a sum right it's nothing complicated it's just applying this logarithmic

1016
01:16:21,520 --> 01:16:25,980
property and the reason why we can draw the logarithm is because this is a monotonic function

1017
01:16:25,980 --> 01:16:32,800
and the reason why we want to do that is this simplifies the whole problem quite a bit

1018
01:16:32,800 --> 01:16:36,600
like mathematically speaking if you're now going back to our linear regression

1019
01:16:36,600 --> 01:16:41,500
um and now what we can do is we can write this

1020
01:16:41,580 --> 01:16:51,200
as a sum instead of a um instead of a product um now we can actually look at what distributions

1021
01:16:51,200 --> 01:16:57,000
um or what probability distributions can our model have and this is our second assumption

1022
01:16:57,000 --> 01:17:01,540
what we're now making right now is we're saying well all of our models that we're doing

1023
01:17:01,540 --> 01:17:08,100
um following or all of our observations what we're what we're what we're looking at

1024
01:17:08,100 --> 01:17:11,420
they all fall follow a gaussian a gaussian distribution

1025
01:17:11,580 --> 01:17:15,180
so what shape does our probability distribution have well we're assuming it's gaussian

1026
01:17:15,740 --> 01:17:24,060
so assuming this is a normal distribution here um we having certain observations these are our y i's

1027
01:17:24,060 --> 01:17:30,140
right we're having our input features times our model parameters and we're having um the variance

1028
01:17:30,140 --> 01:17:34,940
here the standard deviation squared um and now what we can do is we can simply uh pull out the

1029
01:17:34,940 --> 01:17:39,340
mean here right so like here we have zero as our mean and we'll be pulling the mean out

1030
01:17:40,140 --> 01:17:41,580
and we're assuming this is our gaussian

1031
01:17:41,580 --> 01:17:47,740
so it's one over the square root of two pi um times sigma squared times e

1032
01:17:49,260 --> 01:17:57,020
to the power of minus one over two sigma squared times y i minus mu squared and we're assuming that

1033
01:17:57,020 --> 01:18:04,940
our y i's are drawn from this specific distribution right um and what we care about right now is we

1034
01:18:04,940 --> 01:18:10,380
would like to figure out what this term is here in the top what is the respective probability well

1035
01:18:11,580 --> 01:18:14,700
that's the thing where we can now go ahead and take this assumption

1036
01:18:15,340 --> 01:18:21,820
uh so what we've done here before right we have our our likelihood um and now what we can do is

1037
01:18:21,820 --> 01:18:27,180
we can simply go ahead and take this gaussian distribution and for this mu we're just feeding

1038
01:18:27,180 --> 01:18:33,260
in our xi theta right so all they're doing right now is we're simply assuming that our

1039
01:18:35,260 --> 01:18:41,260
our predictions our probabilities here they're gonna follow this gaussian distribution right so

1040
01:18:41,260 --> 01:18:47,340
all we're doing right now is we're just putting in our xi times theta we're putting this one into

1041
01:18:47,340 --> 01:18:52,700
our probabilities right now now right so you might say well okay this makes the term pretty

1042
01:18:52,700 --> 01:18:57,580
complicated but there's a little bit of an advantage where we just had because if we go

1043
01:18:57,580 --> 01:19:03,740
into our actual optimization problem we now know we don't have actual probabilities here we have

1044
01:19:03,740 --> 01:19:10,940
lot probabilities right so what we're trying to do here is we're saying our max over the sum of the

1045
01:19:11,260 --> 01:19:16,220
log probabilities from our model so now what we want to do is we want to put this one here back

1046
01:19:16,220 --> 01:19:22,060
into here and we're taking the logarithm of this whole gaussian here and this is a nice property

1047
01:19:22,060 --> 01:19:27,820
about the logarithm that this simplifies the whole problem quite a bit so the first thing what's going

1048
01:19:27,820 --> 01:19:33,820
to happen is so all i'm doing here right now is i'm putting this one into here this is my first

1049
01:19:35,020 --> 01:19:38,780
row here the first thing you're going to see is well okay we're taking the logarithm here

1050
01:19:39,740 --> 01:19:41,180
of this gaussian so

1051
01:19:41,260 --> 01:19:45,500
there's an e function right so the logarithm and the e function cancel out that's why the

1052
01:19:45,500 --> 01:19:50,940
logarithm is a pretty good choice here why we did this actually in the first place and then

1053
01:19:50,940 --> 01:19:57,100
the next thing what we can do is after so well first after we did that we will see here we

1054
01:19:57,100 --> 01:20:05,820
ending up with one constant here at the front so this one is two pi times sigma squared but

1055
01:20:05,820 --> 01:20:11,020
there's no x and no theta in here so there's no input and no model parameter in here the only

1056
01:20:11,260 --> 01:20:16,540
parameters are here um and then what we can do is we can just rewrite this whole thing as a matrix

1057
01:20:16,540 --> 01:20:20,540
and then this term already becomes a lot simpler right so now we have this constant here at the

1058
01:20:20,540 --> 01:20:29,820
beginning uh that's just a constant and now here we're having uh one over two sigma squared times

1059
01:20:30,620 --> 01:20:36,300
this uh well you guessed it it's a it's the same formula what we had before right it's

1060
01:20:36,300 --> 01:20:41,100
only the regression formula basically um okay and that's all we have right here

1061
01:20:41,260 --> 01:20:47,820
now right um so now what we want to do is if we want to have this log likelihood and we want to

1062
01:20:47,820 --> 01:20:54,060
have our r max here we're ending up with this formula how can we find the estimate of theta

1063
01:20:54,060 --> 01:21:00,300
well we've done this already we're setting we're looking for a linear least squares problem here

1064
01:21:00,300 --> 01:21:04,620
where we're setting the gradient to zero then we're ending up with our linear least squares

1065
01:21:04,620 --> 01:21:09,980
formulation and we try and solve that and then we're having our estimate for our maximum

1066
01:21:11,260 --> 01:21:15,340
likelihood estimate right so this is kind of a very important thing in machine learning that we

1067
01:21:15,340 --> 01:21:22,460
can kind of use this logarithm trick um in order to go from our maximum likelihood estimate um back

1068
01:21:22,460 --> 01:21:26,620
to our linear regression and using the same trick to solve it which is just the linear solve at the

1069
01:21:26,620 --> 01:21:33,020
end of the day okay um there will be some details here in the exercise session specifically regarding

1070
01:21:33,020 --> 01:21:39,180
the math i hope you could still follow it if it's a problem pause the video and go back and have a

1071
01:21:39,180 --> 01:21:41,100
look at the exercises first before you continue on the next one

1072
01:21:41,260 --> 01:21:47,500
so let's continue from here um but as a summary what we have been doing here um the maximum

1073
01:21:47,500 --> 01:21:53,980
likelihood estimator corresponds to the least squares estimate we've just derived that um

1074
01:21:53,980 --> 01:21:57,260
however of course we have to assume these assumptions right we had the assumptions that

1075
01:21:57,260 --> 01:22:03,260
our samples are drawn independent um and that they actually follow a quotient distribution

1076
01:22:03,260 --> 01:22:05,580
these were the two things and they're all the same distribution of course

1077
01:22:06,300 --> 01:22:11,100
um but what's important right now what we have done is we actually made quite a bit

1078
01:22:11,260 --> 01:22:18,540
of progress because now we have had an intro of linear regression and we had an intro of maximum

1079
01:22:18,540 --> 01:22:25,020
likelihood now we also have the concept of a loss function uh we have talked a little bit about

1080
01:22:25,020 --> 01:22:31,740
optimization to obtain the best model for regression um and we can do simple predictions

1081
01:22:31,740 --> 01:22:38,380
right now like the temperature in a room um but what we haven't talked too much about right now is

1082
01:22:38,380 --> 01:22:40,460
we haven't talked about our original task

1083
01:22:41,260 --> 01:22:48,300
our original task is this image classification task um and now the question is can we actually

1084
01:22:48,300 --> 01:22:54,140
do the same trick here also for classification right um so i wanted to reiterate the problem

1085
01:22:54,140 --> 01:22:59,180
statement here um i know some of the things here are redundant and i mentioned it already a little

1086
01:22:59,180 --> 01:23:04,940
bit between the slides so the regression versus the classification the difference here is very

1087
01:23:04,940 --> 01:23:10,540
obvious the the the difference is the regression predicts a continuous output like the temperature

1088
01:23:11,260 --> 01:23:16,380
of the room like you have a floating point value right whereas the classification predicts a

1089
01:23:16,380 --> 01:23:23,820
discrete value like a label um we can have a binary classifier like output is either zero one

1090
01:23:23,820 --> 01:23:27,420
or we can have a multi-class classification which is a set of n classes

1091
01:23:29,580 --> 01:23:34,540
right we're going to have a look at binary classification right now this is the the thing

1092
01:23:34,540 --> 01:23:39,740
that is well the next easy thing to look at um we're not going to talk too much about multi-class

1093
01:23:39,740 --> 01:23:40,860
classification today

1094
01:23:41,260 --> 01:23:47,340
in the next lecture um but this one is i think we want to now go from linear regression to the

1095
01:23:47,340 --> 01:23:53,100
analog of well i guess linear classification would be the main thing but it's it's not quite as simple

1096
01:23:53,100 --> 01:23:59,260
anymore and we want to talk about why it's not as simple anymore the whole method that we're going

1097
01:23:59,260 --> 01:24:04,140
to try to introduce here is called logistic regression and there's a reason why quote unquote

1098
01:24:04,140 --> 01:24:10,460
linear regression doesn't work that well for classification instead we have to do something

1099
01:24:10,460 --> 01:24:11,180
slightly different

1100
01:24:11,260 --> 01:24:16,780
this is what logistic regression is doing um so if we are starting with our linear classifier

1101
01:24:17,740 --> 01:24:23,660
what we would love to do is we would love to predict a probability we would like to make sure

1102
01:24:23,660 --> 01:24:31,340
which probability is it is it this class or is it that class now i mentioned this before a simple

1103
01:24:31,340 --> 01:24:38,460
way to approach this is we could just say well we do a linear regression and if my value is above

1104
01:24:38,460 --> 01:24:41,100
0.5 then it's class one

1105
01:24:41,260 --> 01:24:47,340
if it's below 0.5 then it's class 0. now the problem with that is there's nothing in our

1106
01:24:47,340 --> 01:24:53,900
linear regression that guarantees us that this value will between 0 and 1 and this is what we're

1107
01:24:53,900 --> 01:24:59,420
going to do right now we're going to simply go ahead and do a linear regression which is this

1108
01:24:59,420 --> 01:25:07,260
one here this linear regression here is doing nothing else x 0 times theta 0 x 1 times theta 1

1109
01:25:07,260 --> 01:25:10,860
x 2 times theta 2 sums it all up same thing as before

1110
01:25:11,260 --> 01:25:16,540
right so we have each feature vectors multiplied with one scalar technically we have we have a

1111
01:25:17,180 --> 01:25:21,260
we have a bias here at some point too it's a little inconsistent please forgive me about

1112
01:25:21,260 --> 01:25:25,180
that i but i wanted to make it a little bit easier from a from a writing perspective

1113
01:25:26,540 --> 01:25:28,860
and this is a score function right now

1114
01:25:30,460 --> 01:25:38,860
and we want to we want to just simply feed this output here into a sigmoid and this sigmoid of x

1115
01:25:41,260 --> 01:25:49,580
is 1 over 1 plus e to the power of minus x and the property of this function this function looks

1116
01:25:49,580 --> 01:25:58,380
like this so the point is that this function is between 0 and 1. it's never going to be 0

1117
01:25:58,380 --> 01:26:04,780
it's asymptotically approaching 0 and it's asymptotically approaching 1 right and the

1118
01:26:04,780 --> 01:26:11,180
idea of that is basically that we are squashing the output here between 0 and 1 such that we can

1119
01:26:11,180 --> 01:26:17,020
interpret it as a probability right and this is kind of convenient when it comes to classification

1120
01:26:17,740 --> 01:26:24,140
for instance if we're having a very high outlier here like if this linear classifier would predict

1121
01:26:24,140 --> 01:26:30,060
a very high outlier here it would simply squash it to 1. if it has a very low outlier it would

1122
01:26:30,060 --> 01:26:35,020
simply squash it to 0 and that is for classification a very desired property as we shall see

1123
01:26:36,140 --> 01:26:41,100
okay so what does it mean well um here for instance we

1124
01:26:41,180 --> 01:26:46,540
have the pro the way you would write this is what's the probability of my output being one

1125
01:26:47,820 --> 01:26:53,820
um for given input x given my model parameters data right that's what we couldn't scare about

1126
01:26:53,820 --> 01:27:01,340
like what's the probability that this is the one here um and a small spoiler alert here this is

1127
01:27:01,340 --> 01:27:08,380
basically a one-layer neural network right it's not a deep network um but it's a one-layer neural

1128
01:27:08,380 --> 01:27:11,100
network um and this is the reason why this is a one-layer neural network um and this is the reason why

1129
01:27:11,180 --> 01:27:15,180
we first started with linear regression and now we're going to logistic regression where we have

1130
01:27:15,180 --> 01:27:22,540
the sigma in it as well um i don't want to talk too much about neural networks yet but i obviously

1131
01:27:23,260 --> 01:27:26,380
wanted to give the hint that later when we go into neural networks and you

1132
01:27:26,940 --> 01:27:33,740
miss something go back to this slide here and check out um how the logistic regression works okay

1133
01:27:35,660 --> 01:27:41,100
right so this is logistic regression so how does it work in practice well now what we're

1134
01:27:41,180 --> 01:27:48,380
going to do is we would like to have the output to be a probability and what we'd like to do is

1135
01:27:48,380 --> 01:27:52,220
we would like to connect this to our maximum likelihood estimate what we had done before

1136
01:27:53,180 --> 01:28:03,020
so the probability of our output here um is now defined as follows right so what we're doing is

1137
01:28:03,020 --> 01:28:09,100
um following our logistic uh our maximum likelihood estimator what we had done before um

1138
01:28:10,300 --> 01:28:11,100
what is the question

1139
01:28:11,180 --> 01:28:14,860
what is the probability that all the labels are going to be predicted one

1140
01:28:15,500 --> 01:28:18,460
like what we care about is everything is going to be predicted one

1141
01:28:20,140 --> 01:28:27,100
here we have the product of all of our independent prediction here we have our model parameters here

1142
01:28:27,100 --> 01:28:32,540
we have our inputs and here we have the output labels equal to one and now what we do here is

1143
01:28:33,580 --> 01:28:39,740
we know that this model here that we have is defined as follows we know that our model

1144
01:28:40,460 --> 01:28:45,260
is the prediction of a sigmoid so this here is nothing else as a prediction of a sigmoid

1145
01:28:45,820 --> 01:28:49,980
so y hat i which is the respective prediction of this one thing here

1146
01:28:52,140 --> 01:28:56,140
is actually the sigmoid of this linear model here of x i theta

1147
01:28:56,140 --> 01:29:00,860
again be careful i'm simplifying this a little bit there's also a bias in it in practice right

1148
01:29:00,860 --> 01:29:07,660
so you want to want to also go to this one um and but now what we can do is we can actually go ahead

1149
01:29:08,220 --> 01:29:09,660
and reformulate this

1150
01:29:10,140 --> 01:29:13,500
similar as we have done it before and we can use a bernoulli trial

1151
01:29:14,220 --> 01:29:21,900
so the idea here is that this this prediction here um this phi to the power of z

1152
01:29:22,940 --> 01:29:29,340
times one minus phi to the power of one minus z this is nothing else but a model for coins right

1153
01:29:29,900 --> 01:29:34,220
so give me this input right what's the respective prediction here

1154
01:29:34,860 --> 01:29:38,700
z could be either zero or one this is the respective output in this case it was one but it

1155
01:29:38,700 --> 01:29:39,660
could be zero as well

1156
01:29:40,540 --> 01:29:45,260
and now what we know is we can just take this formula here for our model of the coin predictions

1157
01:29:45,820 --> 01:29:50,620
because this like again this is nothing else but rolling a coin over and over and over again

1158
01:29:51,580 --> 01:29:56,460
and defeating this one into the model of the coins all right so now what we're doing again

1159
01:29:56,460 --> 01:30:01,260
go back here oops we're taking this formula here and we're just putting this one into here

1160
01:30:02,380 --> 01:30:08,620
um so now y hat is nothing else but the product of the predictions of the sigmoid

1161
01:30:09,740 --> 01:30:16,940
all right this one here this y hat i is the sigmoid um this is the respective label of

1162
01:30:16,940 --> 01:30:20,220
our coin predictions again this is this is the model what we have used here

1163
01:30:21,580 --> 01:30:29,580
um and this is the negative part of it right so this is one minus again what the output prediction

1164
01:30:29,580 --> 01:30:37,260
is going to be to the power of one minus the other label and this one here this y i and one minus y i

1165
01:30:37,260 --> 01:30:38,940
is respectively the true

1166
01:30:39,740 --> 01:30:43,580
label could be zero for the first class or it could be one for the second class

1167
01:30:44,940 --> 01:30:49,580
okay um so now we're doing the same thing what we have done for our maximum likelihood estimate

1168
01:30:50,380 --> 01:30:58,780
uh we would like to simplify this uh formulation of probabilities uh this is all probability of

1169
01:30:58,780 --> 01:31:03,580
our binary output and what we would like to do is we would like to reformulate it such that

1170
01:31:03,580 --> 01:31:08,220
we are obtaining a loss function so again to clarify you might ask why on earth are we doing

1171
01:31:08,220 --> 01:31:08,980
this why on earth are we doing this well the whole point is here is if simply if you can find it so you

1172
01:31:08,980 --> 01:31:09,340
are doing it so again to clarify you might ask why on earth are we doing this well the whole point

1173
01:31:09,340 --> 01:31:09,720
is here is that you have the 10 bigidad invalidity eliminate all needed degrees Zent fehl,

1174
01:31:09,720 --> 01:31:16,420
we want to go ahead and get from a probability of a binary output we want to go ahead to our

1175
01:31:16,420 --> 01:31:20,540
loss function and this is what we're doing right now right so now we have this one we're taking

1176
01:31:20,540 --> 01:31:31,680
our maximum likelihood estimate and as we remember my maximum likely estimate is arc max of log p

1177
01:31:31,680 --> 01:31:39,600
y from x and theta and what we would like to do is we would like to make sure we get the maximum

1178
01:31:39,600 --> 01:31:44,740
likelihood estimate here right so we would like to maximize these probabilities okay so now here

1179
01:31:44,740 --> 01:31:50,720
we have our log of p here's our p now we're putting this one into here and that's what's

1180
01:31:50,720 --> 01:31:56,200
going to happen here so here's again our p now we're putting it into this log formulation

1181
01:31:56,200 --> 01:32:01,280
we can do the same trick what we have done before we have this product here of our

1182
01:32:01,280 --> 01:32:08,940
independent predictions now we have the sum of our log predictions and then we're simplifying

1183
01:32:08,940 --> 01:32:09,580
this whole thing a little bit and we're going to do the same thing here and then we're going to

1184
01:32:09,580 --> 01:32:09,600
do the same thing again and then we're simplifying this whole thing a little bit and then we're going

1185
01:32:09,600 --> 01:32:14,300
little bit because the log function and exponential function can be simplified right so we can move

1186
01:32:14,300 --> 01:32:24,540
this y and 1 minus y i we can move this respectively forward um and um yeah and then this is what we're

1187
01:32:24,540 --> 01:32:30,860
getting here right so we have here y i which is our label times the log of our y hat i this is

1188
01:32:30,860 --> 01:32:41,340
our sigmoid prediction uh plus 1 minus y i again it's our label uh times log 1 minus y hat i okay

1189
01:32:41,480 --> 01:32:49,660
there's one little thing um what kind of is relevant right so here's a minus where does

1190
01:32:49,660 --> 01:32:55,400
this minus come from well the point is here this probability here comes from the log likelihood

1191
01:32:55,400 --> 01:33:00,720
from the maximum likelihood estimator um here we would like to maximize the

1192
01:33:00,720 --> 01:33:00,840
function and we would like to maximize the function here so we would like to maximize

1193
01:33:00,840 --> 01:33:00,860
the function and we would like to maximize the function and this would be the maximum

1194
01:33:00,860 --> 01:33:04,620
whereas here we would like to minimize the loss function, right?

1195
01:33:04,620 --> 01:33:09,380
We go in here from our prediction of our probabilities,

1196
01:33:09,640 --> 01:33:11,580
we go in here back to our loss function, right?

1197
01:33:11,700 --> 01:33:16,120
So we want to derive a loss that maximizes the probabilities,

1198
01:33:16,400 --> 01:33:18,160
but the loss itself we want to minimize,

1199
01:33:18,440 --> 01:33:19,880
and this is why we needed this minus here.

1200
01:33:20,800 --> 01:33:20,920
Okay.

1201
01:33:22,740 --> 01:33:23,180
Right.

1202
01:33:23,420 --> 01:33:27,220
And now what we're doing is this is basically our loss function right now.

1203
01:33:27,220 --> 01:33:33,020
So our loss function here is now dependent on the true labels

1204
01:33:33,020 --> 01:33:34,800
and the sigma predictions.

1205
01:33:35,580 --> 01:33:41,600
So we have minus open brackets yi times log y hat i

1206
01:33:41,600 --> 01:33:48,100
plus 1 minus yi times log 1 minus y hat i.

1207
01:33:48,660 --> 01:33:50,340
Now, what do we know here?

1208
01:33:50,700 --> 01:33:54,300
Well, what we want to do here is basically we want to see

1209
01:33:54,300 --> 01:33:56,440
what happens if our label is 1.

1210
01:33:56,440 --> 01:33:57,200
Like, why does this happen?

1211
01:33:57,240 --> 01:33:58,460
Does it actually work what we're doing here?

1212
01:33:58,540 --> 01:34:00,680
So if our ground truth label is 1,

1213
01:34:01,160 --> 01:34:04,220
that means this term here will be 0, right?

1214
01:34:04,300 --> 01:34:05,620
So this term cancels out,

1215
01:34:05,920 --> 01:34:12,380
and our loss will simply be the loss of y hat i in 1 is simply log yi.

1216
01:34:14,520 --> 01:34:15,880
Sorry, there's the minus missing.

1217
01:34:16,000 --> 01:34:17,200
This minus must be here as well.

1218
01:34:19,040 --> 01:34:20,500
And then what we can do again,

1219
01:34:21,660 --> 01:34:26,680
we can go ahead and say, well, we can prove that this is true, right?

1220
01:34:26,680 --> 01:34:26,760
Right.

1221
01:34:26,760 --> 01:34:26,820
Right.

1222
01:34:26,820 --> 01:34:26,880
Right.

1223
01:34:26,880 --> 01:34:26,900
Right.

1224
01:34:26,900 --> 01:34:26,920
Right.

1225
01:34:26,920 --> 01:34:27,180
Right.

1226
01:34:27,200 --> 01:34:30,760
So what we want to do is we want to maximize the argmax of thetas

1227
01:34:30,760 --> 01:34:35,060
such that this is the maximum likelihood estimator, right?

1228
01:34:35,340 --> 01:34:40,740
Or to say differently, we want log y hat to be large

1229
01:34:40,740 --> 01:34:44,100
since the logarithm is a monotonically increasing function.

1230
01:34:44,800 --> 01:34:49,060
And because of that, we also want to be y hat i to be large, right?

1231
01:34:49,240 --> 01:34:52,080
So if this one is large, then this one is, sorry,

1232
01:34:52,220 --> 01:34:54,560
if this one is large, then this one is large and vice versa.

1233
01:34:56,400 --> 01:34:56,920
And this is the maximum likelihood estimator.

1234
01:34:56,920 --> 01:35:05,340
The thing that's important here is actually that one is actually the largest value of our model's estimate that it can take.

1235
01:35:06,100 --> 01:35:06,500
Why?

1236
01:35:07,860 --> 01:35:09,200
Well, we have a sigmoid function.

1237
01:35:09,420 --> 01:35:11,060
That's the whole point of it, right?

1238
01:35:12,760 --> 01:35:15,960
And if we're doing it the other way around, we're setting this one to zero,

1239
01:35:16,620 --> 01:35:19,120
then this one cancels out because this one will be zero.

1240
01:35:19,340 --> 01:35:20,360
Only this one will remain.

1241
01:35:20,640 --> 01:35:23,660
And then our loss function will be this one here.

1242
01:35:24,080 --> 01:35:25,440
I apologize for this mistake.

1243
01:35:25,440 --> 01:35:26,660
There must be a minus here, right?

1244
01:35:26,660 --> 01:35:28,780
This minus here should be here as well, respectively.

1245
01:35:30,400 --> 01:35:30,840
Okay.

1246
01:35:31,520 --> 01:35:33,300
Now, if you're putting these two things together,

1247
01:35:34,400 --> 01:35:35,860
this is our loss function.

1248
01:35:36,120 --> 01:35:38,500
We know that it can be summarized like that

1249
01:35:38,500 --> 01:35:43,860
because we just done the differentiation between the minus and,

1250
01:35:44,680 --> 01:35:49,220
sorry, the zero label case and the one label case.

1251
01:35:49,640 --> 01:35:52,900
And this is something what's called the binary cross entropy loss.

1252
01:35:52,900 --> 01:35:55,200
It's called the BCE loss, right?

1253
01:35:55,200 --> 01:35:59,780
And this is something that we will see very, very many times in this lecture.

1254
01:35:59,780 --> 01:36:08,520
We will also use the same loss in neural network land and later also expand it to the multi-class loss,

1255
01:36:08,520 --> 01:36:10,020
which is called softmax.

1256
01:36:10,020 --> 01:36:12,020
So you will see this also a couple of times.

1257
01:36:12,560 --> 01:36:15,800
Now, how do we actually define our cost function?

1258
01:36:15,800 --> 01:36:18,700
Well, now we have to go ahead and sum all of this stuff up here.

1259
01:36:19,540 --> 01:36:24,320
And in practice, what this means is we take this loss function, we just sum it up.

1260
01:36:24,320 --> 01:36:24,740
Okay.

1261
01:36:24,740 --> 01:36:32,260
Everywhere over all the samples, we have n samples and then there's a little one over in here.

1262
01:36:33,140 --> 01:36:37,300
Technically speaking, if we would be really precise,

1263
01:36:37,300 --> 01:36:42,300
we would also have had this one over in here in this loss function here as well,

1264
01:36:42,300 --> 01:36:44,820
because that's part of the maximum likelihood estimate, right?

1265
01:36:44,820 --> 01:36:52,740
So each sample is part of a distribution and this distribution is multiplied with the respective probability

1266
01:36:52,740 --> 01:36:54,740
appearing as part of the entire distribution.

1267
01:36:54,740 --> 01:36:56,740
So this is the entirety of the samples.

1268
01:36:56,740 --> 01:37:00,740
But I simplified this a little bit because I wanted to make my notation easier.

1269
01:37:00,740 --> 01:37:06,740
So just take it as we just averaging here over the individual loss values from each of the samples.

1270
01:37:06,740 --> 01:37:08,740
Okay.

1271
01:37:08,740 --> 01:37:10,740
What we do, well, we're minimizing our cost function.

1272
01:37:10,740 --> 01:37:12,740
We minimizing that one.

1273
01:37:12,740 --> 01:37:14,740
This is our minimization.

1274
01:37:14,740 --> 01:37:22,740
And again, as a brief reminder, y hat i is sigmoid of x i times theta, linear part, nonlinear part.

1275
01:37:22,740 --> 01:37:24,740
Okay.

1276
01:37:24,740 --> 01:37:28,740
Now we're going to see already we have to optimize that, right?

1277
01:37:28,740 --> 01:37:32,740
So now we have to also, again, optimize for our theta.

1278
01:37:32,740 --> 01:37:36,740
And we will already see that this theta here is part of a sigmoid right now,

1279
01:37:36,740 --> 01:37:42,740
which unfortunately makes this whole problem a little bit more intense to optimize.

1280
01:37:42,740 --> 01:37:46,740
So there's no closed-form solution anymore.

1281
01:37:46,740 --> 01:37:48,740
So we need to use an iterative method how to solve it.

1282
01:37:48,740 --> 01:37:52,740
And this iterative method is gradient descent.

1283
01:37:52,740 --> 01:37:54,740
Gradient descent, again,

1284
01:37:54,740 --> 01:37:56,740
we will talk about this a little bit in the exercises.

1285
01:37:56,740 --> 01:37:58,740
If you don't know it,

1286
01:37:58,740 --> 01:38:00,740
you should really look it up.

1287
01:38:00,740 --> 01:38:02,740
This is really critical.

1288
01:38:02,740 --> 01:38:04,740
A lot of this stuff will be based on gradient descent, what we're doing here.

1289
01:38:04,740 --> 01:38:06,740
And this is going to be very, very, very critical.

1290
01:38:08,740 --> 01:38:10,740
There will be

1291
01:38:10,740 --> 01:38:12,740
a little bit of more in the lecture as well.

1292
01:38:12,740 --> 01:38:14,740
We're going to talk about different solvers.

1293
01:38:14,740 --> 01:38:16,740
The one thing

1294
01:38:16,740 --> 01:38:18,740
what I wanted to mention here at this point, though,

1295
01:38:18,740 --> 01:38:20,740
is that gradient descent

1296
01:38:20,740 --> 01:38:22,740
is actually

1297
01:38:22,740 --> 01:38:24,740
or gradient-based optimization

1298
01:38:24,740 --> 01:38:26,740
to begin with,

1299
01:38:26,740 --> 01:38:28,740
is really key

1300
01:38:28,740 --> 01:38:30,740
to a lot of the machine learning methods.

1301
01:38:30,740 --> 01:38:32,740
There's also a philosophical question is

1302
01:38:32,740 --> 01:38:34,740
does machine learning

1303
01:38:34,740 --> 01:38:36,740
or this learning-based

1304
01:38:36,740 --> 01:38:38,740
life, in a sense,

1305
01:38:38,740 --> 01:38:40,740
or human life,

1306
01:38:40,740 --> 01:38:42,740
do we learn based on gradients?

1307
01:38:42,740 --> 01:38:44,740
I don't want to make the connection all the time,

1308
01:38:44,740 --> 01:38:46,740
but I think this was kind of a remarkable question

1309
01:38:46,740 --> 01:38:48,740
of how do

1310
01:38:48,740 --> 01:38:50,740
physical neurons actually optimize?

1311
01:38:50,740 --> 01:38:52,740
Because the assumption was

1312
01:38:52,740 --> 01:38:54,740
they are similar in a sense.

1313
01:38:54,740 --> 01:38:56,740
This is not so clear yet.

1314
01:38:56,740 --> 01:38:58,740
But what we can tell is that all neural networks

1315
01:38:58,740 --> 01:39:00,740
are gradient-based, and so far

1316
01:39:00,740 --> 01:39:02,740
specific gradient-based

1317
01:39:02,740 --> 01:39:04,740
optimizations

1318
01:39:04,740 --> 01:39:06,740
have shown really remarkable progress.

1319
01:39:06,740 --> 01:39:08,740
And that's what we're going to build on.

1320
01:39:08,740 --> 01:39:10,740
And I think that is

1321
01:39:10,740 --> 01:39:12,740
pretty cool and pretty exciting.

1322
01:39:12,740 --> 01:39:14,740
So, if we're summarizing today's

1323
01:39:14,740 --> 01:39:16,740
lecture, I think

1324
01:39:16,740 --> 01:39:18,740
the reason, I mean, I'm really excited

1325
01:39:18,740 --> 01:39:20,740
about all of this, because I think

1326
01:39:20,740 --> 01:39:22,740
obviously machine learning is really cool.

1327
01:39:22,740 --> 01:39:24,740
The idea is kind of we learn

1328
01:39:24,740 --> 01:39:26,740
from experience, right? So we're giving it more data,

1329
01:39:26,740 --> 01:39:28,740
and we don't have to deal with like

1330
01:39:28,740 --> 01:39:30,740
hand-drafting a lot of stuff anymore.

1331
01:39:30,740 --> 01:39:32,740
But we're learning from experiences

1332
01:39:32,740 --> 01:39:34,740
and can then kind of make

1333
01:39:34,740 --> 01:39:36,740
well, somewhat intelligent

1334
01:39:36,740 --> 01:39:38,740
predictions.

1335
01:39:38,740 --> 01:39:40,740
And to infer

1336
01:39:40,740 --> 01:39:42,740
something about the future. And I think this is really

1337
01:39:42,740 --> 01:39:44,740
useful for a lot of different tasks.

1338
01:39:44,740 --> 01:39:46,740
It could be very simple tasks,

1339
01:39:46,740 --> 01:39:48,740
and to be

1340
01:39:48,740 --> 01:39:50,740
pretty frank, a lot of the problems

1341
01:39:50,740 --> 01:39:52,740
you can actually solve with linear regression.

1342
01:39:52,740 --> 01:39:54,740
Like, linear regression is a

1343
01:39:54,740 --> 01:39:56,740
really powerful tool that you

1344
01:39:56,740 --> 01:39:58,740
can apply for many, many problem

1345
01:39:58,740 --> 01:40:00,740
statements, and they are probably

1346
01:40:00,740 --> 01:40:02,740
pretty good. So if a linear regressor

1347
01:40:02,740 --> 01:40:04,740
well, a linear regressor

1348
01:40:04,740 --> 01:40:06,740
should always be a good baseline, you should try first.

1349
01:40:06,740 --> 01:40:08,740
So,

1350
01:40:08,740 --> 01:40:10,740
in linear models, for instance

1351
01:40:10,740 --> 01:40:12,740
in complex phenomena, like weather

1352
01:40:12,740 --> 01:40:14,740
or so, can actually be quite good.

1353
01:40:14,740 --> 01:40:16,740
And

1354
01:40:16,740 --> 01:40:18,740
there are certain reasons why that is the case.

1355
01:40:18,740 --> 01:40:20,740
We're going to talk about this a little bit.

1356
01:40:20,740 --> 01:40:22,740
Neural networks are always the best choice,

1357
01:40:22,740 --> 01:40:24,740
but nowadays with a couple

1358
01:40:24,740 --> 01:40:26,740
of iterations now,

1359
01:40:26,740 --> 01:40:28,740
yeah, we have seen

1360
01:40:28,740 --> 01:40:30,740
the neural networks, we can tweak them quite well as well.

1361
01:40:30,740 --> 01:40:32,740
And that's really exciting,

1362
01:40:32,740 --> 01:40:34,740
but I hope as a summary

1363
01:40:34,740 --> 01:40:36,740
maybe of this lecture, like really take away

1364
01:40:36,740 --> 01:40:38,740
what machine learning has,

1365
01:40:38,740 --> 01:40:40,740
take away that machine learning is always

1366
01:40:40,740 --> 01:40:42,740
the very same concept. Namely,

1367
01:40:42,740 --> 01:40:44,740
you're going to go ahead,

1368
01:40:44,740 --> 01:40:46,740
you take a training data set,

1369
01:40:46,740 --> 01:40:48,740
from this training data set, you're going to fit a model

1370
01:40:48,740 --> 01:40:50,740
to it, you're trying to approximate the

1371
01:40:50,740 --> 01:40:52,740
distribution of a training set, and you

1372
01:40:52,740 --> 01:40:54,740
hope it's going to generalize to some answer.

1373
01:40:54,740 --> 01:40:56,740
It's always the same thing.

1374
01:40:56,740 --> 01:40:58,740
Now, of course, the devil here is in the detail.

1375
01:40:58,740 --> 01:41:00,740
Like, what's your model? What's your optimizer?

1376
01:41:00,740 --> 01:41:02,740
What's your data?

1377
01:41:02,740 --> 01:41:04,740
How do you optimize it well?

1378
01:41:04,740 --> 01:41:06,740
How do you regularize it?

1379
01:41:06,740 --> 01:41:08,740
There's going to be a lot of details we have to talk about

1380
01:41:08,740 --> 01:41:10,740
how to make this work well. And people have struggled with this

1381
01:41:10,740 --> 01:41:12,740
over the last five or

1382
01:41:12,740 --> 01:41:14,740
well, four decades basically

1383
01:41:14,740 --> 01:41:16,740
to get it to work to a reasonable level.

1384
01:41:16,740 --> 01:41:18,740
And I'm sure there's also going to be new stuff coming up.

1385
01:41:18,740 --> 01:41:20,740
But the backbone of all of this

1386
01:41:20,740 --> 01:41:22,740
is math and statistics.

1387
01:41:22,740 --> 01:41:24,740
And that makes it pretty exciting,

1388
01:41:24,740 --> 01:41:26,740
because it's on one hand quite theoretical,

1389
01:41:26,740 --> 01:41:28,740
but it's also very applied.

1390
01:41:28,740 --> 01:41:30,740
And you can actually show super, super

1391
01:41:30,740 --> 01:41:32,740
cool results here.

1392
01:41:32,740 --> 01:41:34,740
Okay, with that, I'm going to end this lecture.

1393
01:41:34,740 --> 01:41:36,740
In the next exercise session,

1394
01:41:36,740 --> 01:41:38,740
we're going to talk about the math recap part two.

1395
01:41:38,740 --> 01:41:40,740
I mentioned this already here

1396
01:41:40,740 --> 01:41:42,740
a couple of times as part of this lecture.

1397
01:41:42,740 --> 01:41:44,740
If anything is unclear,

1398
01:41:44,740 --> 01:41:46,740
please ask in the Piazza forum

1399
01:41:46,740 --> 01:41:48,740
or, I mean,

1400
01:41:48,740 --> 01:41:50,740
both ideally,

1401
01:41:50,740 --> 01:41:52,740
definitely watch the math recaps

1402
01:41:52,740 --> 01:41:54,740
and attend the math recaps.

1403
01:41:54,740 --> 01:41:56,740
In the next lecture,

1404
01:41:56,740 --> 01:41:58,740
we're going to talk about

1405
01:41:58,740 --> 01:42:00,740
the first neural networks,

1406
01:42:00,740 --> 01:42:02,740
we're going to talk about computational graphs,

1407
01:42:02,740 --> 01:42:04,740
and we're going to expand on what we have learned today.

1408
01:42:04,740 --> 01:42:06,740
A few references for further reading.

1409
01:42:08,740 --> 01:42:10,740
Generally speaking,

1410
01:42:10,740 --> 01:42:12,740
maybe I'll start at the bottom here.

1411
01:42:12,740 --> 01:42:14,740
General machine learning.

1412
01:42:14,740 --> 01:42:16,740
This lecture is not necessarily

1413
01:42:16,740 --> 01:42:18,740
the focus of machine learning to begin with.

1414
01:42:18,740 --> 01:42:20,740
But I wanted to still explain

1415
01:42:20,740 --> 01:42:22,740
the main concepts.

1416
01:42:22,740 --> 01:42:24,740
We're leaving a lot of other machine learning methods

1417
01:42:24,740 --> 01:42:26,740
out there.

1418
01:42:26,740 --> 01:42:28,740
I'm not going to talk too much about

1419
01:42:28,740 --> 01:42:30,740
things like support vector machines

1420
01:42:30,740 --> 01:42:32,740
or random decision forests.

1421
01:42:32,740 --> 01:42:34,740
They're also very important.

1422
01:42:34,740 --> 01:42:36,740
But that's not the main focus right now.

1423
01:42:36,740 --> 01:42:38,740
They're different models.

1424
01:42:38,740 --> 01:42:40,740
They have their own strengths.

1425
01:42:40,740 --> 01:42:42,740
One recommendation is

1426
01:42:42,740 --> 01:42:44,740
if you're interested in machine learning,

1427
01:42:44,740 --> 01:42:46,740
look at this book by Chris Bishop.

1428
01:42:46,740 --> 01:42:48,740
That's a very, very powerful book

1429
01:42:48,740 --> 01:42:50,740
generally speaking to learn

1430
01:42:50,740 --> 01:42:52,740
certain math concepts.

1431
01:42:52,740 --> 01:42:54,740
In this lecture,

1432
01:42:54,740 --> 01:42:56,740
in the book, they explain much better.

1433
01:42:56,740 --> 01:42:58,740
The other thing,

1434
01:42:58,740 --> 01:43:00,740
I wanted to talk a little bit about this one,

1435
01:43:00,740 --> 01:43:02,740
but I cut it out of the lecture itself,

1436
01:43:02,740 --> 01:43:04,740
was how to split training

1437
01:43:04,740 --> 01:43:06,740
validation and test sets

1438
01:43:06,740 --> 01:43:08,740
and how to make use of that.

1439
01:43:08,740 --> 01:43:10,740
There's a concept called cross-validation.

1440
01:43:10,740 --> 01:43:12,740
I would recommend having a look at this one too

1441
01:43:12,740 --> 01:43:14,740
because that helps you to find better hyperparameters.

1442
01:43:14,740 --> 01:43:16,740
It might not be

1443
01:43:16,740 --> 01:43:18,740
directly relevant

1444
01:43:18,740 --> 01:43:20,740
right away for the linear models,

1445
01:43:20,740 --> 01:43:22,740
but when you start

1446
01:43:22,740 --> 01:43:24,740
training your personal networks,

1447
01:43:24,740 --> 01:43:26,740
these things are actually quite useful

1448
01:43:26,740 --> 01:43:28,740
to know because they're actually going to help you

1449
01:43:28,740 --> 01:43:30,740
to get better results on your maths.

1450
01:43:30,740 --> 01:43:32,740
All right.

1451
01:43:32,740 --> 01:43:34,740
With that, we're going to be finished today.

1452
01:43:34,740 --> 01:43:36,740
Thanks a lot for attending

1453
01:43:36,740 --> 01:43:38,740
and I hope to see you for the next lecture.

1454
01:43:38,740 --> 01:43:40,740
Thanks everybody for joining.

